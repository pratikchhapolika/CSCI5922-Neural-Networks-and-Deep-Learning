{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Optimization\n",
    "\n",
    "***\n",
    "**Name**: __Poorwa Hirve__\n",
    "***\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this assignment is to use gradient based optimization algorithms to find the minum for the Rosenbrock function and to optmizie the 1 layer MLP network you built in **Assignment 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimizing the Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information can be found here https://en.wikipedia.org/wiki/Rosenbrock_function\n",
    "    \n",
    "**Note: For this assignment, we will choose a = 1 and b = 100**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $f(x,y) = (a-x)^2 + b \\cdot (y - x^2)^2 $  \n",
    "\n",
    "\n",
    "- $\\frac{\\partial{f(x,y)}}{\\partial{x}} = 2 \\cdot (a-x) \\cdot (-1) + b \\cdot 2 \\cdot (y - x^2) \\cdot (-1) \\cdot 2 \\cdot x  $\n",
    "\n",
    "\n",
    "- $\\frac{\\partial{f(x,y)}}{\\partial{y}} = b \\cdot 2 \\cdot (y - x^2)  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed \n",
    "import random\n",
    "import unittest\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(a, b, x, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    a,b : parameters \n",
    "    x, y: inputs\n",
    "    \n",
    "    Outputs:\n",
    "    out: Rosenbrock function evaluated at x and y\n",
    "    \"\"\"\n",
    "    out = (a - x)**2 + b*(y - x**2)**2\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_grad(a, b, x, y):\n",
    "    \"\"\"\n",
    "    Calculate gradient of the rosenbrock function wrt x and y\n",
    "    \n",
    "    Inputs:\n",
    "    a, b: parameters\n",
    "    x, y: inputs\n",
    "    \n",
    "    Outputs:\n",
    "    grad_x, grad_y: Gradients wrt x and y\n",
    "    \"\"\"\n",
    "    grad_x = -2*(a - x) - 4*b*(y - x**2)*x\n",
    "    grad_y = 2*b*(y - x**2)\n",
    "    return grad_x, grad_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you are given skeleton code for optimizing the rosenbrock function using various update rules. Following each function, there are a few function calls with specific hyperparameter choices. The outputs for these will be used to grade your work.\n",
    "\n",
    "Termination condition: \n",
    "\n",
    "1. Reached the n_epochs limit \n",
    "\n",
    "2. The change in value of the function at $x_t,y_t$ and $x_{t+1},y_{t+1}$ is <= tolerance \n",
    "\n",
    "All these functions share the same structure i.e apart from the update rule (and keeping track of past variables) very little changes across these functions.\n",
    "\n",
    "Note: since there is no randomness involved, we expect the outputs to closely match those of our implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_sgd(initial_x, initial_y, a, b, n_epochs, lr, tolerance):\n",
    "    \"\"\"\n",
    "    Use Vanilla SGD to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: Learning rate\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    out_prev = -math.inf\n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    stop_epoch = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        out = rosenbrock(a, b, final_x, final_y)\n",
    "        if abs(out - out_prev) <= tolerance:\n",
    "            return final_x, final_y, stop_epoch\n",
    "        out_prev = out\n",
    "        grad_x, grad_y = rosenbrock_grad(a, b, final_x, final_y)\n",
    "        final_x = final_x - lr * grad_x\n",
    "        final_y = final_y - lr * grad_y\n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x, final_y, n_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_momentum(initial_x, initial_y, a, b, n_epochs, lr, mntm, nesterov, tolerance):\n",
    "    \"\"\"\n",
    "    Use momentum to optimize the Rosenbrock function\n",
    "    \n",
    "    Tip: While implementing nesterov update, you will need the gradient at the next step as well. \n",
    "        Instead, to simplify your implementation, you can use an alternative form of the nesterov \n",
    "        update which only uses the gradient at the current step.\n",
    "        Without nesterov, your update will be -> learning_rate*(gradient + momentum*grad_history)\n",
    "        With nesterov, you update will be -> lr*((1+mntm)*gradient + (mntm)^2 * grad_history)\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: Learning rate\n",
    "    mntm: momentum factor\n",
    "    nesterov: True if nesterov update is to be used\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    out_prev = -math.inf\n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    stop_epoch = 0\n",
    "    if nesterov:\n",
    "        _grad = 1 + mntm\n",
    "        _grad_hist = mntm**2\n",
    "        \n",
    "    else:\n",
    "        _grad = 1\n",
    "        _grad_hist = mntm\n",
    "        \n",
    "    grad_hist_x, grad_hist_y = 0,0\n",
    "            \n",
    "    for epoch in range(n_epochs):\n",
    "        out = rosenbrock(a, b, final_x, final_y)\n",
    "        if abs(out - out_prev) <= tolerance:\n",
    "            return final_x, final_y, stop_epoch\n",
    "        out_prev = out\n",
    "        grad_x, grad_y = rosenbrock_grad(a, b, final_x, final_y)\n",
    "        \n",
    "        grad_hist_x = lr * (_grad*grad_x + _grad_hist*grad_hist_x)\n",
    "        grad_hist_y = lr * (_grad*grad_y + _grad_hist*grad_hist_y)\n",
    "        \n",
    "        final_x = final_x - grad_hist_x\n",
    "        final_y = final_y - grad_hist_y\n",
    "\n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x, final_y, n_epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_adagrad(initial_x, initial_y, a, b, n_epochs, lr, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use Adagrad to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: Learning rate\n",
    "    eps: The fudge factor (used in the denominator of the update to reduce numerical instability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    out_prev = -math.inf\n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    r_x = 0\n",
    "    r_y = 0\n",
    "    stop_epoch = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        out = rosenbrock(a, b, final_x, final_y)\n",
    "        if abs(out - out_prev) <= tolerance:\n",
    "            return final_x, final_y, stop_epoch\n",
    "        out_prev = out\n",
    "        grad_x, grad_y = rosenbrock_grad(a, b, final_x, final_y)\n",
    "        r_x = r_x + grad_x**2\n",
    "        r_y = r_y + grad_y**2\n",
    "        final_x = final_x - lr * grad_x / ((eps+r_x)**0.5)\n",
    "        final_y = final_y - lr * grad_y / ((eps+r_y)**0.5)\n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x, final_y, n_epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_adadelta(initial_x, initial_y, a, b, n_epochs, rho, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use Adadelta to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    rho: Averaging factor\n",
    "    eps: fudging factor (for numerical stability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    out_prev = -math.inf\n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    stop_epoch = 0\n",
    "    prev_avg_x = 0\n",
    "    prev_avg_y = 0\n",
    "    prev_delta_x = 0\n",
    "    prev_delta_y = 0\n",
    "    update_x = update_y = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        out = rosenbrock(a, b, final_x, final_y)\n",
    "        if abs(out - out_prev) <= tolerance:\n",
    "            return final_x, final_y, stop_epoch\n",
    "        out_prev = out\n",
    "        grad_x, grad_y = rosenbrock_grad(a, b, final_x, final_y)\n",
    "        \n",
    "        avg_x = rho * prev_avg_x + (1 - rho) * grad_x**2\n",
    "        avg_y = rho * prev_avg_y + (1 - rho) * grad_y**2\n",
    "        \n",
    "        delta_x = rho * prev_delta_x + (1 - rho) * (update_x)**2\n",
    "        delta_y = rho * prev_delta_y + (1 - rho) * (update_y)**2\n",
    "        \n",
    "        prev_avg_x = avg_x\n",
    "        prev_avg_y = avg_y  \n",
    "        \n",
    "        update_x = -(delta_x + eps)**0.5 * grad_x / ((avg_x + eps)**0.5)\n",
    "        update_y = -(delta_y + eps)**0.5 * grad_y / ((avg_y + eps)**0.5)\n",
    "        \n",
    "        final_x = final_x + update_x\n",
    "        final_y = final_y + update_y\n",
    "        \n",
    "        prev_delta_x = delta_x\n",
    "        prev_delta_y = delta_y\n",
    "        \n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x, final_y, n_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_rmsprop(initial_x, initial_y, a, b, n_epochs, lr, rho, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use RMSprop to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr:learning rate\n",
    "    rho: Averaging factor\n",
    "    eps: fudging factor (for numerical stability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    r_x = r_y = 0\n",
    "    out_prev = -math.inf\n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    stop_epoch = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        out = rosenbrock(a, b, final_x, final_y)\n",
    "        if abs(out - out_prev) <= tolerance:\n",
    "            return final_x, final_y, stop_epoch\n",
    "        out_prev = out\n",
    "        grad_x, grad_y = rosenbrock_grad(a, b, final_x, final_y)\n",
    "        \n",
    "        r_x = rho * r_x + (1 - rho) * grad_x**2\n",
    "        r_y = rho * r_y + (1 - rho) * grad_y**2\n",
    "        \n",
    "        final_x = final_x - lr * grad_x / (eps+(r_x**0.5))\n",
    "        final_y = final_y - lr * grad_y / (eps+(r_y**0.5))\n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x, final_y, n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_adam(initial_x, initial_y, a, b, n_epochs, lr, beta1, beta2, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use Adam to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: learning rate\n",
    "    beta1, beta2: Averaging factors\n",
    "    eps: fudging factor (for numerical stability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    out_prev = -math.inf\n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    stop_epoch = 0\n",
    "    s_x = s_y = r_x = r_y = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        out = rosenbrock(a, b, final_x, final_y)\n",
    "        if abs(out - out_prev) <= tolerance:\n",
    "            return final_x, final_y, stop_epoch\n",
    "        out_prev = out\n",
    "        grad_x, grad_y = rosenbrock_grad(a, b, final_x, final_y)\n",
    "        \n",
    "        s_x = (beta1 * s_x) + ((1 - beta1) * grad_x)\n",
    "        s_y = (beta1 * s_y) + ((1 - beta1) * grad_y)\n",
    "        \n",
    "        r_x = (beta2 * r_x) + ((1 - beta2) * grad_x ** 2)\n",
    "        r_y = (beta2 * r_y) + ((1 - beta2) * grad_y ** 2)\n",
    "        \n",
    "        _s_x = s_x / (1 - beta1**(epoch+1))\n",
    "        _s_y = s_y / (1 - beta1**(epoch+1))\n",
    "        \n",
    "        _r_x = r_x / (1 - beta2**(epoch+1))\n",
    "        _r_y = r_y / (1 - beta2**(epoch+1))\n",
    "        \n",
    "        final_x = final_x - lr * _s_x / (eps+(_r_x**0.5))\n",
    "        final_y = final_y - lr * _s_y / (eps+(_r_y**0.5))\n",
    "\n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x, final_y, n_epochs\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.053s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestRosenBrock(unittest.TestCase):\n",
    "    def test_sgd(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_sgd(0, 0, 1, 100, 1, 0.001, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.002, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "\n",
    "        final_x, final_y, stop_epoch = rosenbrock_sgd(0, 0, 1, 100, 5, 0.001, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.009959805751775453, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 2.091e-05, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_sgd(0, 0, 1, 100, 100000, 0.001, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.965628504058544, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.932297997695398, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch,  5570, places = 4)\n",
    "\n",
    "    def test_momentum(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 1, 0.001, 0.95, True, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0039, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 5, 0.001, 0.95, True, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.019358983472059735, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.00013646707993741637, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 100000, 0.001, 0.95, True, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.975228639507472, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9509702874332484, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 3253, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 1, 0.001, 0.95, False, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.002, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 5, 0.001, 0.95, False, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.00996736498329375, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 2.0949769804179422e-05, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 100000, 0.001, 0.95, False, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9656473450926458, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9323344634810155, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5566, places = 4)\n",
    "\n",
    "    def test_adagrad(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adagrad(0, 0, 1, 100, 1, lr = 0.01, eps = 1e-04, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.009999875002343702, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adagrad(0, 0, 1, 100, 5, lr = 0.01, eps = 1e-04, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0321538388386626, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.0007748244054554124, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adagrad(0, 0, 1, 100, 100000, lr = 0.01, eps = 1e-04, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9601029668996042, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9217502990713288, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 8105, places = 4)\n",
    "\n",
    "    def test_adadelta(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adadelta(0, 0, 1, 100, 1, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0004472135843196183, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adadelta(0, 0, 1, 100, 5, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0022774214255509655, places = 4)\n",
    "        self.assertAlmostEqual(final_y, -0.00020407559768761523, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adadelta(0, 0, 1, 100, 10000, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9320850940444182, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.8726938644568353, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 2341, places = 4)\n",
    "\n",
    "    def test_rmsprop(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_rmsprop(0, 0, 1, 100, 1, lr = 0.001, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.004472135843196183, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_rmsprop(0, 0, 1, 100, 5, lr = 0.001, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.01471318674752176, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.00016279213371358814, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_rmsprop(0, 0, 1, 100, 100000, lr = 0.001, rho = 0.95, eps = 1e-08, tolerance = 1e-08)\n",
    "        self.assertAlmostEqual(final_x, 0.9876128078500009, places = 4)\n",
    "        self.assertAlmostEqual(final_y,  0.9768279433532969, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 3172, places = 4)\n",
    "\n",
    "    def test_adam(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adam(0, 0, 1, 100, 1, lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.000999999995, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adam(0, 0, 1, 100, 5, lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.004999562994105255, places = 4)\n",
    "        self.assertAlmostEqual(final_y, -0.0005982751561081573, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adam(0, 0, 1, 100, 10000, lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9882985356765371, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9767314100355338, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 2191, places = 4)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor import Tensor\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, n_in, n_hid, n_out):\n",
    "\n",
    "        self.l1 = Tensor(np.random.randn(n_in, n_hid))\n",
    "        self.l2 = Tensor(np.random.randn(n_hid, n_out))\n",
    "        \n",
    "        self.state = defaultdict(dict)\n",
    "        \n",
    "        # Initialize an empty dictionary (per pair of weights and biases) for future use\n",
    "        # This dictionary will store various histories needed int he algorithms to be implemented\n",
    "        for i in [1, 2]:\n",
    "            self.state[str(i)] = {}\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Implement the forward pass.\n",
    "\n",
    "            params:\n",
    "            X (NxM Tensor):\n",
    "                M dimensional input to be feed forward to the network.\n",
    "\n",
    "            returns:\n",
    "\n",
    "            y_hat (1xN Tensor): activations of the output layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.act_l1 = X.dot(self.l1).relu()\n",
    "        self.act_l2 = self.act_l1.dot(self.l2)\n",
    "\n",
    "        return self.act_l2\n",
    "\n",
    "    def update(self, opt):\n",
    "        \"\"\"\n",
    "            Implement the update rule for network weights and biases.\n",
    "\n",
    "            return:\n",
    "\n",
    "            none.\n",
    "        \"\"\"\n",
    "        \n",
    "        grad_1, grad_2 = opt.update_grad(self)\n",
    "        self.l1.value += grad_1.value\n",
    "        self.l2.value += grad_2.value\n",
    "        \n",
    "    def get_state_accum(self, param, state_dict, name):\n",
    "        \"\"\"\n",
    "        Return a particular variable associated with a layer. If it has not been initialized \n",
    "        (i.e. on the first iteration), then create that variable with the same dimention as param\n",
    "\n",
    "        Example usage: get_state_accum(W, nn.state[str(layer)], 'W_grad_history') would return the \n",
    "                       gradient in the past iteration (assuming you update W_grad_history in the dictionary every\n",
    "                       epoch)\n",
    "\n",
    "        Inputs:\n",
    "        state_dict: dictionary to be queried \n",
    "        name: property of interest\n",
    "        param: If state_dict[name] is being accessed the first time, we initialize it to be a \n",
    "               vector with the same dimesions as param\n",
    "        \"\"\"\n",
    "        if name not in state_dict:\n",
    "            state_dict[name] = np.zeros_like(param)\n",
    "        return state_dict[name]\n",
    "    \n",
    "    def get_layer_params(self, layer):\n",
    "        \"\"\"\n",
    "        Return aparameters of a particular layer\n",
    "        \"\"\"\n",
    "        if layer == 1:\n",
    "            return self.l1\n",
    "        else:\n",
    "            return self.l2\n",
    "        \n",
    "    def get_layer_grad(self, layer):\n",
    "        \"\"\" Return gradient of a particular layer \"\"\"\n",
    "        if layer == 1:\n",
    "            return self.l1.grad\n",
    "        else:\n",
    "            return self.l2.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "            Resets gradients for each layer defined in\n",
    "            the constructor.\n",
    "        \"\"\"\n",
    "        self.l1.zero_grad()\n",
    "        self.l2.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer():\n",
    "    def __init__(self, update_rule, **kwargs):\n",
    "        \"\"\"\n",
    "        Set self.update \n",
    "        Set parameters from kwargs(variable keyword parameters) according to update type\n",
    "        \"\"\"\n",
    "        self.update = update_rule\n",
    "        \n",
    "        if self.update == \"sgd\":\n",
    "            self.lr = kwargs['lr']\n",
    "        elif self.update == \"momentum\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.mntm = kwargs['mntm']\n",
    "            self.nesterov = kwargs['nesterov']\n",
    "        elif self.update == \"adagrad\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.eps = kwargs['eps']\n",
    "        elif self.update == \"adadelta\":\n",
    "            self.rho = kwargs['rho']\n",
    "            self.eps = kwargs['eps']\n",
    "        elif self.update == \"rmsprop\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.rho = kwargs['rho']\n",
    "            self.eps = kwargs['eps']\n",
    "        elif self.update == \"adam\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.beta1 = kwargs['beta1']\n",
    "            self.beta2 = kwargs['beta2']\n",
    "            self.eps = kwargs['eps']\n",
    "    \n",
    "    def update_grad(self, model):\n",
    "        \"\"\"\n",
    "        Returns proper update according to the string in self.update\n",
    "        \n",
    "        Input:\n",
    "        model: NN model\n",
    "        \n",
    "        Output:\n",
    "        l1: Update for layer 1\n",
    "        l2: update for layer 2\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.update == \"sgd\":\n",
    "            l1 = self.update_sgd(1, model, self.lr)\n",
    "            l2 = self.update_sgd(2, model, self.lr)\n",
    "        elif self.update == \"momentum\":\n",
    "            l1 = self.update_momentum(1, model, self.lr, self.mntm, self.nesterov)\n",
    "            l2 = self.update_momentum(2, model, self.lr, self.mntm, self.nesterov)\n",
    "        elif self.update == \"adagrad\":\n",
    "            l1 = self.update_adagrad(1, model, self.lr, self.eps)\n",
    "            l2 = self.update_adagrad(2, model, self.lr, self.eps)\n",
    "        elif self.update == \"adadelta\":\n",
    "            l1 = self.update_adadelta(1, model, self.rho, self.eps)\n",
    "            l2 = self.update_adadelta(2, model, self.rho, self.eps)\n",
    "        elif self.update == \"rmsprop\":\n",
    "            l1 = self.update_rmsprop(1, model, self.lr, self.rho, self.eps)\n",
    "            l2 = self.update_rmsprop(2, model, self.lr, self.rho, self.eps)\n",
    "        elif self.update == \"adam\":\n",
    "            l1 = self.update_adam(1, model, self.lr, self.beta1, self.beta2, self.eps)\n",
    "            l2 = self.update_adam(2, model, self.lr, self.beta1, self.beta2, self.eps)\n",
    "        \n",
    "        return l1, l2\n",
    "        \n",
    "    def update_sgd(self, layer, model, lr):\n",
    "        \"\"\"\n",
    "        Update function for sgd.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "\n",
    "        layer_grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        update = -lr * layer_grad\n",
    "        \n",
    "        model.state[str(layer)]['sgd_update'] = update   \n",
    "        \n",
    "        return Tensor(update)\n",
    "        \n",
    "    \n",
    "    def update_momentum(self, layer, model, lr, mntm, nesterov):\n",
    "        \"\"\"\n",
    "        Update function for momentum.\n",
    "        Make sure you update the past gradients for 'layer' inside this function.\n",
    "\n",
    "        Tip: While implementing nesterov update, you will need the gradient at the next step as well. \n",
    "            Instead, to simplify your implementation, you can use an alternative form of the nesterov \n",
    "            update which only uses the gradient at the current step.\n",
    "            Without nesterov, your update will be -> learning_rate*(gradient + momentum*grad_history)\n",
    "            With nesterov, you update will be -> lr*((1+mntm)*gradient + (mntm)^2 * grad_history)\n",
    "            \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        mntm: momentum factor\n",
    "        nesterov: if True, function returns nesterov accelerated update\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "                \n",
    "        if nesterov:\n",
    "            name = 'nesterov_momentum_gradient'\n",
    "            _grad = 1 + mntm\n",
    "            _grad_hist = mntm**2\n",
    "        else:\n",
    "            name = 'momentum_gradient'\n",
    "            _grad = 1\n",
    "            _grad_hist = mntm\n",
    "            \n",
    "        grad_history = model.get_state_accum(model.get_layer_grad(layer), model.state[str(layer)], name)\n",
    "        \n",
    "        layer_grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        grad_history = -lr * (_grad*layer_grad + _grad_hist*grad_history)\n",
    "        \n",
    "        model.state[str(layer)][name] = grad_history\n",
    "\n",
    "        return Tensor(grad_history)\n",
    "    \n",
    "    def update_adagrad(self, layer, model, lr, eps):\n",
    "        \"\"\"\n",
    "        Update function for adadelta.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "                \n",
    "        r_history = model.get_state_accum(model.get_layer_grad(layer), model.state[str(layer)], 'adagrad_r_history')\n",
    "        \n",
    "        layer_grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        r = r_history + layer_grad**2\n",
    "        \n",
    "        model.state[str(layer)]['adagrad_r_history'] = r\n",
    "        \n",
    "        return Tensor(-lr * layer_grad / ((eps+r)**0.5))\n",
    "            \n",
    "    def update_adadelta(self, layer, model, rho, eps):\n",
    "        \"\"\"\n",
    "        Update function for adadelta.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        rho: averaging factor\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        avg_history = model.get_state_accum(model.get_layer_grad(layer), model.state[str(layer)], 'adadelta_avg_history')\n",
    "        \n",
    "        delta_history = model.get_state_accum(model.get_layer_grad(layer), model.state[str(layer)], 'adadelta_delta_history')\n",
    "        \n",
    "        update_history = model.get_state_accum(model.get_layer_grad(layer), model.state[str(layer)], 'adadelta_update_history')\n",
    "        \n",
    "        layer_grad = model.get_layer_grad(layer)\n",
    "                \n",
    "        avg = rho * avg_history + (1 - rho) * layer_grad**2\n",
    "        \n",
    "        update_history = -(delta_history + eps)**0.5 * layer_grad / ((avg + eps)**0.5)\n",
    "        \n",
    "        delta = rho * delta_history + (1 - rho) * update_history**2\n",
    "        \n",
    "        update = -(delta + eps)**0.5 * layer_grad / ((avg + eps)**0.5)\n",
    "                \n",
    "        model.state[str(layer)]['adadelta_avg_history'] = avg\n",
    "        \n",
    "        model.state[str(layer)]['adadelta_delta_history'] = delta        \n",
    "        \n",
    "        return Tensor(update)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def update_rmsprop(self, layer, model, lr, rho, eps):\n",
    "        \"\"\"\n",
    "        Update function for rmsprop.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        rho : averaging factor\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        \n",
    "        r_history = model.get_state_accum(model.get_layer_grad(layer), model.state[str(layer)], 'rmsprop_r_history')\n",
    "        \n",
    "        layer_grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        r = rho * r_history + (1 - rho) * layer_grad**2\n",
    "        \n",
    "        model.state[str(layer)]['rmsprop_r_history'] = r\n",
    "        \n",
    "        return Tensor(-lr * layer_grad / (eps+(r**0.5)))\n",
    "        \n",
    "    \n",
    "    def update_adam(self, layer, model, lr, beta1, beta2, eps):\n",
    "        \"\"\"\n",
    "        Update function for adam.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        beta1, beta2 : averaging factors\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        \n",
    "        s_history = model.get_state_accum(model.get_layer_grad(layer), model.state[str(layer)], 'adam_s_history')\n",
    "        \n",
    "        r_history = model.get_state_accum(model.get_layer_grad(layer), model.state[str(layer)], 'adam_r_history')\n",
    "        \n",
    "        epoch_history = model.get_state_accum(1, model.state[str(layer)], 'adam_epoch_history')\n",
    "        \n",
    "        layer_grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        s = (beta1 * s_history) + ((1 - beta1) * layer_grad)\n",
    "        \n",
    "        r = (beta2 * r_history) + ((1 - beta2) * layer_grad**2)\n",
    "        \n",
    "        _s = s / (1 - beta1**(epoch_history+1))\n",
    "        _r = r / (1 - beta2**(epoch_history+1))\n",
    "        \n",
    "        model.state[str(layer)]['adam_s_history'] = s\n",
    "        model.state[str(layer)]['adam_r_history'] = r\n",
    "        model.state[str(layer)]['adam_epoch_history'] += 1\n",
    "        \n",
    "        return Tensor(-lr * _s / (eps+(_r**0.5)))\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, epochs, batch_size, opt):\n",
    "    \"\"\"\n",
    "    Implement the train loop.\n",
    "    \n",
    "    params:\n",
    "    model (MLP): 1-hidden-layer MLP model to be trained.\n",
    "    X (Nx5 ndarray): Training inputs.\n",
    "    y (Nx1 ndarray): Groundtruth labels.\n",
    "    epochs (int): number of epochs for training\n",
    "    batch_size (int)\n",
    "    opt: optimizer object\n",
    "    \n",
    "    returns:\n",
    "    list of errors (one for each epoch)\n",
    "    \"\"\"\n",
    "    # For each minibatch in each epoch, \n",
    "    # 1. do a forward pass on the minibatch\n",
    "    # 2. Compute Loss on minibatch\n",
    "    # 3. Do a backward pass to get gradients\n",
    "    # 4. Update parameters\n",
    "    # 5. Compute error over epoch\n",
    "    epoch_list = []\n",
    "    \n",
    "    batches = int(X.shape[0] / batch_size)\n",
    "    for i in range(epochs):\n",
    "        X, y = shuffle(X, y)\n",
    "        batch_error = []\n",
    "        \n",
    "        for j in range(0, X.shape[0], batch_size):\n",
    "            y_hat = model.forward(Tensor(X[j:j+batch_size]))\n",
    "            model.zero_grad()\n",
    "            y_actual = Tensor(y[j:j+batch_size])\n",
    "            \n",
    "            error = (y_hat - y_actual).pow(2).mean()\n",
    "            batch_error.append(error.value)\n",
    "            error.backward()\n",
    "            \n",
    "            model.update(opt)\n",
    "            \n",
    "        epoch_list.append(sum(batch_error) / batches)\n",
    "        \n",
    "    return epoch_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Usage and Tests\n",
    "***\n",
    "\n",
    "**Part 3.A**  \n",
    "\n",
    "Create a model with $I = 5$ inputs, $H = 100$ and $O = 1$ output neuron. Train the model for 100 epochs using the dataset below. Plot the MSE loss for each epoch.\n",
    "\n",
    "Once you have each optimizer working, which performs best given their initial hyperparameters? Does changing one or more hyperpameters yield better performance for a particular optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.load(\"X.npy\"), np.load(\"y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36964405]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"sgd\", lr = 0.001)\n",
    "sgd_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "print (sgd_epoch_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36230076]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"momentum\", lr = 0.001, mntm = 0.95, nesterov = False)\n",
    "momentum_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "print (momentum_epoch_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2546077]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"momentum\", lr = 0.001, mntm = 0.95, nesterov = True)\n",
    "nesterov_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "print (nesterov_epoch_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18041726]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"adagrad\", lr = 0.1, eps = 1e-06)\n",
    "adagrad_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "print (adagrad_epoch_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23440233]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"adadelta\", rho = 0.9, eps = 1e-06)\n",
    "adadelta_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "print (adadelta_epoch_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18288123]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"rmsprop\", lr = 0.001, rho = 0.9, eps = 1e-06)\n",
    "rmsprop_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "print (rmsprop_epoch_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98779893]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"adam\", lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-04)\n",
    "adam_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "print (adam_epoch_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115371b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = range(100)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, sgd_epoch_list, label='sgd')\n",
    "plt.plot(x, momentum_epoch_list, label='momentum')\n",
    "plt.plot(x, nesterov_epoch_list, label='nesterov')\n",
    "plt.plot(x, adagrad_epoch_list, label='adagrad')\n",
    "plt.plot(x, adadelta_epoch_list, label='adadelta')\n",
    "plt.plot(x, rmsprop_epoch_list, label='rmsprop')\n",
    "plt.plot(x, adam_epoch_list, label='adam')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Adagrad__ performs the best with initial hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.001, 0.01, 0.1]\n",
    "mntm_list = [0.85, 0.9, 0.95]\n",
    "rho_list = [0.85, 0.9, 0.95]\n",
    "beta1_list = [0.9, 0.95, 0.99]\n",
    "beta2_list = [0.9, 0.95, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15604457]\n",
      "Optimum Learning Rate for SGD: 0.01\n"
     ]
    }
   ],
   "source": [
    "### SGD\n",
    "\n",
    "min_sgd_list = [100]*100\n",
    "min_sgd_lr = 0\n",
    "\n",
    "for lr in lr_list:\n",
    "    model = MLP(X.shape[1], 100, 1)\n",
    "    opt = optimizer(\"sgd\", lr = lr)\n",
    "    _sgd_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "    if _sgd_epoch_list[-1] < min_sgd_list[-1]:\n",
    "        min_sgd_list = _sgd_epoch_list\n",
    "        min_sgd_lr = lr\n",
    "\n",
    "print (min_sgd_list[-1])\n",
    "print (\"Optimum Learning Rate for SGD:\", min_sgd_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14691101]\n",
      "Optimum Learning Rate for Momentum: 0.01\n",
      "Optimum Momentum Term for Momentum: 0.9\n"
     ]
    }
   ],
   "source": [
    "### MOMENTUM\n",
    "\n",
    "min_momentum_list = [100]*100\n",
    "min_momentum_lr = 0\n",
    "min_momentum_mntm = 0\n",
    "\n",
    "for lr in lr_list:\n",
    "    for mntm in mntm_list:\n",
    "        model = MLP(X.shape[1], 100, 1)\n",
    "        opt = optimizer(\"momentum\", lr = lr, mntm = mntm, nesterov = False)\n",
    "        _momentum_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "        if _momentum_epoch_list[-1] < min_momentum_list[-1]:\n",
    "            min_momentum_list = _momentum_epoch_list\n",
    "            min_momentum_lr = lr\n",
    "            min_momentum_mntm = mntm\n",
    "\n",
    "print (min_momentum_list[-1])\n",
    "print (\"Optimum Learning Rate for Momentum:\", min_momentum_lr)\n",
    "print (\"Optimum Momentum Term for Momentum:\", min_momentum_mntm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25311725]\n",
      "Optimum Learning Rate for Nesterov Momentum: 0.001\n",
      "Optimum Momentum Term for Nesterov Momentum: 0.95\n"
     ]
    }
   ],
   "source": [
    "### NESTEROV\n",
    "\n",
    "min_nesterov_list = [100]*100\n",
    "min_nesterov_lr = 0\n",
    "min_nesterov_mntm = 0\n",
    "\n",
    "for lr in lr_list:\n",
    "    for mntm in mntm_list:\n",
    "        model = MLP(X.shape[1], 100, 1)\n",
    "        opt = optimizer(\"momentum\", lr = lr, mntm = mntm, nesterov = True)\n",
    "        _nesterov_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "        if _nesterov_epoch_list[-1] < min_nesterov_list[-1]:\n",
    "            min_nesterov_list = _nesterov_epoch_list\n",
    "            min_nesterov_lr = lr\n",
    "            min_nesterov_mntm = mntm\n",
    "            \n",
    "print (min_nesterov_list[-1])\n",
    "print (\"Optimum Learning Rate for Nesterov Momentum:\", min_nesterov_lr)\n",
    "print (\"Optimum Momentum Term for Nesterov Momentum:\", min_nesterov_mntm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17506184]\n",
      "Optimum Learning Rate for Adagrad: 0.1\n"
     ]
    }
   ],
   "source": [
    "### ADAGRAD\n",
    "\n",
    "min_adagrad_list = [100]*100\n",
    "min_adagrad_lr = 0\n",
    "\n",
    "for lr in lr_list:\n",
    "    model = MLP(X.shape[1], 100, 1)\n",
    "    opt = optimizer(\"adagrad\", lr = lr, eps = 1e-06)\n",
    "    _adagrad_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "    if _adagrad_epoch_list[-1] < min_adagrad_list[-1]:\n",
    "        min_adagrad_list = _adagrad_epoch_list\n",
    "        min_adagrad_lr = lr\n",
    "        \n",
    "print (min_adagrad_list[-1])\n",
    "print (\"Optimum Learning Rate for Adagrad:\", min_adagrad_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18373093]\n",
      "Optimum Rho for Adadelta: 0.95\n"
     ]
    }
   ],
   "source": [
    "### ADADELTA\n",
    "\n",
    "min_adadelta_list = [100]*100\n",
    "min_adadelta_rho = 0\n",
    "\n",
    "for lr in lr_list:\n",
    "    for rho in rho_list:\n",
    "        model = MLP(X.shape[1], 100, 1)\n",
    "        opt = optimizer(\"adadelta\", rho = rho, eps = 1e-06)\n",
    "        _adadelta_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "        if _adadelta_epoch_list[-1] < min_adadelta_list[-1]:\n",
    "            min_adadelta_list = _adadelta_epoch_list\n",
    "            min_adadelta_rho = rho\n",
    "            \n",
    "print (min_adadelta_list[-1])\n",
    "print (\"Optimum Rho for Adadelta:\", min_adadelta_rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16937058]\n",
      "Optimum Learning Rate for RMSProp: 0.001\n",
      "Optimum Rho for RMSProp: 0.9\n"
     ]
    }
   ],
   "source": [
    "### RMSPROP\n",
    "\n",
    "min_rmsprop_list = [100]*100\n",
    "min_rmsprop_lr = 0\n",
    "min_rmsprop_rho = 0\n",
    "\n",
    "for lr in lr_list:\n",
    "    for rho in rho_list:\n",
    "        model = MLP(X.shape[1], 100, 1)\n",
    "        opt = optimizer(\"rmsprop\", lr = lr, rho = rho, eps = 1e-06)\n",
    "        _rmsprop_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "        if _rmsprop_epoch_list[-1] < min_rmsprop_list[-1]:\n",
    "            min_rmsprop_list = _rmsprop_epoch_list\n",
    "            min_rmsprop_lr = lr\n",
    "            min_rmsprop_rho = rho\n",
    "            \n",
    "print (min_rmsprop_list[-1])\n",
    "print (\"Optimum Learning Rate for RMSProp:\", min_rmsprop_lr)\n",
    "print (\"Optimum Rho for RMSProp:\", min_rmsprop_rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14795103]\n",
      "Optimum Learning Rate for Adam: 0.01\n",
      "Optimum Beta1 for Adam: 0.95\n",
      "Optimum Beta2 for Adam: 0.99\n"
     ]
    }
   ],
   "source": [
    "### ADAM\n",
    "\n",
    "min_adam_list = [100]*100\n",
    "min_adam_lr = 0\n",
    "min_adam_beta1 = 0\n",
    "min_adam_beta2 = 0\n",
    "\n",
    "for lr in lr_list:\n",
    "    for beta1 in beta1_list:\n",
    "        for beta2 in beta2_list:\n",
    "            model = MLP(X.shape[1], 100, 1)\n",
    "        opt = optimizer(\"adam\", lr = lr, beta1 = beta1, beta2 = beta2, eps = 1e-04)\n",
    "        _adam_epoch_list = train(model, X, y, 100, 100, opt)\n",
    "        if _adam_epoch_list[-1] < min_adam_list[-1]:\n",
    "            min_adam_list = _adam_epoch_list\n",
    "            min_adam_lr = lr\n",
    "            min_adam_beta1 = beta1\n",
    "            min_adam_beta2 = beta2\n",
    "            \n",
    "print (min_adam_list[-1])\n",
    "print (\"Optimum Learning Rate for Adam:\", min_adam_lr)\n",
    "print (\"Optimum Beta1 for Adam:\", min_adam_beta1)\n",
    "print (\"Optimum Beta2 for Adam:\", min_adam_beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11543b048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.xlim(0,50)\n",
    "plt.plot(x, min_sgd_list, label='sgd')\n",
    "plt.plot(x, min_momentum_list, label='momentum')\n",
    "plt.plot(x, min_nesterov_list, label='nesterov')\n",
    "plt.plot(x, min_adagrad_list, label='adagrad')\n",
    "plt.plot(x, min_adadelta_list, label='adadelta')\n",
    "plt.plot(x, min_rmsprop_list, label='rmsprop')\n",
    "plt.plot(x, min_adam_list, label='adam')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As seen in the above graph, the optimizers performance improves when experimenting with the hyperparameters and choosing the best one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
