{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "***\n",
    "**Name**: __Poorwa Hirve__\n",
    "***\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this assignment is to build a one-hidden-layer back propagation network to process real data.  For this assignment you will implement the neural net (activation function and training code) yourself, not using tensorflow or other deep learning frameworks. The purpose is for you to understand the nitty gritty of what these frameworks are doing for you before we switch over to using them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "***\n",
    "\n",
    "In this assignment you will be using the Occupancy Detection data set (ODD). It consists of experimental data used for binary classification of room occupancy (i.e., room is occupied versus empty) based on temperature, humidity, light, and CO2 sensors. The train and test data sets are each collected over a week period.\n",
    "\n",
    "The data set includes time stamps with date and hour/minute/second within the day. You are **not to use time stamp features** for predicting occupancy. Since this is a commercial office building, the time stamp is a strong predictor of occupancy. Rather, the goal is to determine whether occupancy - **occ** can be sensed from:\n",
    "1. temperature, expressed in degrees Celsius - **temp**\n",
    "2. relative humidity, expressed as a % - **hum**\n",
    "3. light, in lux - **light**\n",
    "4. CO2, in ppm - **cdx**\n",
    "5. humidity ratio, which is derived from the temperature and the relative humidity - **hum_ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/datatraining.txt')\n",
    "X = data[['date', 'Humidity', 'Light', 'CO2', 'HumidityRatio', 'Temperature']].values\n",
    "y = data['Occupancy'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "incorrect_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Perceptron\n",
    "***\n",
    "\n",
    "Using the perceptron code you wrote for Assignment 1, train a perceptron (linear activation function with a binary threshold) using the training set. Your perceptron should have the 5 input variables described above.\n",
    "\n",
    "**Part A**: Report the training and test set performance in terms of % examples classified correctly.\n",
    "\n",
    "**NOTE**: The Perceptron Update Rule is guaranteed to converge only if there is a setting of the weights that will classify the training set perfectly.  (The learning rule minimizes mistakes. When all examples are classified correctly, the weights stop changing.)  With a noisy data set like this one, the algorithm will not find an exact solution.  Also remember that the perceptron algorithm is not performing gradient descent. Instead, it will jitter around the ideal solution continually changing the weights from one iteration to the next. The weight changes will have a small effect on performance, so you'll see training set performance jitter a bit as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1449.6823999998633 389.79666666669027 94.52166666656652\n",
      " -0.42321991670248976 -7182.8963000003305]\n",
      "219\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "def step(Xw):\n",
    "    \"\"\"\n",
    "    Returns 1 if the dot product of weight and features is > 0 else returns 0\n",
    "    \"\"\"\n",
    "    if Xw > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def perceptron_update(X, y, w, alpha):\n",
    "    incorrect = 0\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        y_hat = step(w.dot(X[i]))\n",
    "        w = w + alpha*(y[i] - y_hat) * X[i]\n",
    "        if y_hat != y[i]:\n",
    "            incorrect += 1\n",
    "    \n",
    "    return w, incorrect\n",
    "\n",
    "def perceptron(X, y, maxIter, alpha):\n",
    "    w = np.zeros_like(X[0])\n",
    "    for it in range(maxIter):\n",
    "        X, y = shuffle(X, y)\n",
    "        w, incorrect = perceptron_update(X, y, w, alpha) \n",
    "        incorrect_list.append(incorrect)\n",
    "        if it > 100 and incorrect_list[-2] > 0 and abs(incorrect_list[-1] - incorrect_list[-2]) / incorrect_list[-2] <= 0.001:\n",
    "            break\n",
    "            \n",
    "    return w, incorrect, it+1\n",
    "\n",
    "w, n, i = perceptron(X_train.T[1:].T, y_train, 1000, 0.1)\n",
    "print(w)\n",
    "print(n)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9701146288209607\n"
     ]
    }
   ],
   "source": [
    "print (\"Train accuracy:\", 1 - float(n) / float(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, y, w):\n",
    "    \"\"\"\n",
    "    Use this function to classify examples in the test set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Test set features\n",
    "    y : Test set labels\n",
    "    w : Perceptron coefficients\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    correct : number of correctly classified examples\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for i in range(len(y)):\n",
    "        y_hat = step(w.dot(X[i]))\n",
    "        if y_hat == y[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct\n",
    "\n",
    "n = classify(X_test.T[1:].T, y_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9889570552147239\n"
     ]
    }
   ],
   "source": [
    "print (\"Test accuracy:\", float(n) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 1-layer feedforward neural net\n",
    "***\n",
    "\n",
    "In this part, you will implement a 1-layer feedforward neural network from scratch as shown in the figure below.\n",
    "<img src=\"./res/mlp_ann.png\" alt=\"mlp_ann\" style=\"width:500px;\"/>\n",
    "Your tasks are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 2.A**\n",
    "\n",
    "1. Implement `sigmoid(x)`: In this assignment, we will be using the sigmoid activation function as the non-linearity.\n",
    "2. Implement `forward(x)` which calculates the activations at layer 1, 2 and 3 (stored in variables `_a01`, `_a12`, `_a23` respectively) and returns the final layer activations.\n",
    "3. Implement `backward(y, y_hat)` such that it returns layer 1 and layer 2 gradients using the mean squared loss between the predicted and groundtruth vectors.\n",
    "4. Implement `update(l1_grad, l2_grad)` function which will update the weights and biases stored in `_W1, _W2 and _b1, _b2`.\n",
    "5. Implement`train(X, y, epochs)`.\n",
    "6. Implement `predict(X)` to return one-hot encoded representation of the `_a23` layer activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndg = np.random.RandomState(seed=0)\n",
    "epoch_list = []\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"\n",
    "        -- Skeleton Code for a Multi-Layer Perceptron Neural Net.\n",
    "           Network has 1 input layer followed by a hidden layer and\n",
    "           an output layer. \n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, n_vis=5, n_hid=10, n_out=1, batch_size=7, lr=0.1):\n",
    "        \"\"\"\n",
    "            Initialize an MLP object.\n",
    "            \n",
    "            params\n",
    "            \n",
    "            n_vis (int): # of neurons in input layer.\n",
    "            n_hid (int): # of neurons in hidden layer.\n",
    "            n_out (int): # number of neurons in final output layer.\n",
    "            batch_size (int): # number of X,y instances in a mini-batch.\n",
    "            lr (double): learning rate/step size.\n",
    "\n",
    "            class variables:\n",
    "            \n",
    "            _W1, _b1 (np.ndarray): \n",
    "                Weights and biases of the input layer respectively.\n",
    "            \n",
    "            _W2, _b2 (np.ndarray):\n",
    "                Weights and biases of the hidden layer respectively.\n",
    "            \n",
    "            _a01, _a12, _a23 (np.ndarray or None):\n",
    "                Activations from input, hidden and output layer respectively.  \n",
    "        \"\"\"\n",
    "        self._n_vis = n_vis\n",
    "        self._n_hid = n_hid\n",
    "        self._n_out = n_out\n",
    "        \n",
    "        self._W1 = rndg.normal(size=(self._n_hid, self._n_vis))\n",
    "        self._b1 = np.ones((self._n_hid, 1))\n",
    "\n",
    "        self._W2 = rndg.normal(size=(self._n_out, self._n_hid))\n",
    "        self._b2 = np.ones((self._n_out, 1))\n",
    "\n",
    "        self._a01 = None\n",
    "        self._a12 = None\n",
    "        self._a23 = None\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "            Sigmoid Logistic Function.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X (np.ndarray)\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            result (np.ndarray): result = f(X) \n",
    "                where f is the sigmoid function.\n",
    "            \n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-X.astype(float)))        \n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Feed forward an input through each \n",
    "            of the layers of network.\n",
    "            \n",
    "            Store each layer activation in the class \n",
    "            variables defined in the constructor. You'll\n",
    "            need them later during backpropogation.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X (np.ndarray): batch_size x 5 dimensional array\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            y_hat (np.ndarray): 1 x batch_size dimensional array\n",
    "                representing final layer outputs.\n",
    "            \n",
    "        \"\"\"\n",
    "        self._a01 = X.T\n",
    "        _z12 = np.dot(self._W1, self._a01) + self._b1\n",
    "        \n",
    "        self._a12 = self.sigmoid(_z12)\n",
    "        _z23 = np.dot(self._W2, self._a12) + self._b2\n",
    "        \n",
    "        self._a23 = self.sigmoid(_z23)\n",
    "        y_hat = self._a23\n",
    "        \n",
    "        return self._a23\n",
    "\n",
    "    def backward(self, y, y_hat):\n",
    "        \n",
    "        \"\"\"\n",
    "            Implement the backpropogation algorithm.\n",
    "            Assume mean squared loss at the output.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            y (np.ndarray): batch_size x 1 dimensional vector\n",
    "                of ground truth labels.\n",
    "            y_hat (np.nd_array): 1 x 7 dimensional vector\n",
    "                of predicted lables from forward().\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            l1_grad (np.ndarray): 1 x n_hid dimensional array\n",
    "                representing gradients for input layer.\n",
    "                \n",
    "            l2_grad (np.ndarray): n_hid x n_vis dimensional array\n",
    "                representing gradients for hidden layer.\n",
    "            \n",
    "        \"\"\"\n",
    "                \n",
    "        delta = (y_hat - y) * (y_hat*(1.0 - y_hat))\n",
    "\n",
    "        _a = self._a12\n",
    "        l2_grad = delta.dot(_a.T)\n",
    "        \n",
    "        delta_a = self._W2.T.dot(delta)\n",
    "        delta_sig = _a*(1.0 - _a)\n",
    "        delta = np.multiply(delta_a, delta_sig)\n",
    "        \n",
    "        _a = self._a01\n",
    "        l1_grad = delta.dot(_a.T)\n",
    "        \n",
    "        return l1_grad, l2_grad\n",
    "                \n",
    "    def update(self, l1_grad, l2_grad):\n",
    "        \"\"\"\n",
    "            Implement the update rule for network weights and biases.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            l1_grad (np.ndarray): gradients for input layer.\n",
    "            l2_grad (np.ndarray): gradients for hidden layer.\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            none.\n",
    "        \"\"\"\n",
    "        # TO-DO\n",
    "        self._W1 = self._W1 - self.lr * l1_grad\n",
    "        self._W2 = self._W2 - self.lr * l2_grad\n",
    "        \n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        \"\"\"\n",
    "            Returns one hot encoding of vector X using 0.5\n",
    "            as the threshold.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X (np.ndarray): predictions/activations of the output layer.\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            y (np.ndarray): one hot encoding of output layer.\n",
    "        \"\"\"\n",
    "        y_hat = self.forward(X)\n",
    "        for i in range(len(X)):\n",
    "            if y_hat[0][i] >= 0.5:\n",
    "                y_hat[0][i] = 1\n",
    "            else:\n",
    "                y_hat[0][i] = 0\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def train(self, X, y, epochs=1):\n",
    "        \"\"\"\n",
    "            Implement the train loop for n epochs.\n",
    "            In each epoch do the following:\n",
    "            1. Shuffle the dataset.\n",
    "            2. create self.batch_size sized-mini-batches of the dataset.\n",
    "            3. get network predictions using forward().\n",
    "            4. calculate the gradients using backward().\n",
    "            5. update the network weights using update().\n",
    "            6. Repeat 1-5 until convergence.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X (np.ndarray): N x 5 dimensional ndarray of inputs.\n",
    "            y (np.ndarray): N x 1 dimensional array of true labels.\n",
    "            epochs (int): # of epochs to train the network for. \n",
    "            \n",
    "            return:\n",
    "            \n",
    "            none.\n",
    "        \"\"\"\n",
    "        # TO-DO\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            X, y = shuffle(X, y)\n",
    "            \n",
    "            for j in range(0, X.shape[0], batch_size):\n",
    "                y_hat = self.forward(X[j:j+batch_size])\n",
    "                l1_grad, l2_grad = self.backward(y[j:j+batch_size], y_hat)\n",
    "                self.update(l1_grad, l2_grad)\n",
    "            \n",
    "            epoch_list.append(self.test(X_test, y_test))\n",
    "            \n",
    "    def test(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "            Implement the test function which reports accuracy of \n",
    "            the model on the test set X_test against the true labels\n",
    "            y_test.\n",
    "            \n",
    "            You may re-use the predict and accuracy functions defined above.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X_test (np.ndarray): N x M dimensional array where M is the \n",
    "                number of attributes considered for training the model.\n",
    "            y_test (np.ndarray): N x 1 dimensional array.\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            accuracy (double): accuracy of the predicted labels against the \n",
    "                groundtruth labels.\n",
    "        \"\"\"\n",
    "        y = self.predict(X_test)\n",
    "        return self.accuracy(y_test, y)\n",
    "    \n",
    "    @classmethod\n",
    "    def accuracy(self, y, y_hat):\n",
    "        len_ = y.flatten().shape[0]\n",
    "        return np.sum(y.flatten() == y_hat.flatten())/len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.463s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def load_params(path, model):\n",
    "    \n",
    "    params = pickle.load(open(path, 'rb'))\n",
    "    for key in params:\n",
    "        model.__dict__[key] = params[key]\n",
    "\n",
    "class TestMLP(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        \n",
    "        self.net = MLP()\n",
    "        self.pre_trained_model = load_params(\"./res/mlp.pkl\", self.net)\n",
    "        self.X = np.load(\"./res/x.npy\")\n",
    "        self.y = np.load(\"./res/y.npy\")\n",
    "        self.p = np.load(\"./res/p.npy\")\n",
    "        self.sigX = np.asarray([-1, -2, -3, 0, 1, 2, 3])\n",
    "    \n",
    "    def test_forward(self):\n",
    "        self.assertEqual(np.sum(self.net.forward(self.X[901:])), np.sum(self.p))\n",
    "    \n",
    "    def test_train(self):\n",
    "        x, y = make_classification(\n",
    "            n_samples=1000,\n",
    "            n_features=5,\n",
    "            n_redundant=0,\n",
    "            random_state=rndg\n",
    "        )\n",
    "        net = MLP(5, 5, 1, 7, 0.01)\n",
    "        net.train(self.X[:900], self.y[:900], 100)\n",
    "        self.assertTrue(net.accuracy(net.predict(self.X[901:]), self.y[901:]) > 0.88)\n",
    "    \n",
    "    def test_sigmoid(self):\n",
    "        self.assertEqual(\n",
    "            np.sum(self.net.sigmoid(self.sigX)), 3.5\n",
    "        )\n",
    "        \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestMLP)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 2.B**\n",
    "\n",
    "Report the baseline performance of the untrained MLP that you built in **PART 2.A** using the selected values. To calculate the baseline score, feed-forward entire test set through the test function. The test function will return a set of predictions. Calculate the accuracy of these predictions against ground truth training lables.  \n",
    "Report:\n",
    "1. accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20736196319018405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "### Part 2.B\n",
    "mlp = MLP()\n",
    "baseline_score = mlp.test(X_test.T[1:].T, y_test)\n",
    "print (baseline_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 2.C**\n",
    "\n",
    "Select a learning rate, batch size and train the network for 100 epochs. After each epoch plot a graph of network's accuracy on the test data. On the same graph, plot a constant horizontal line of the baseline score that you reported in **PART 2.B**.  \n",
    "\n",
    "Report:\n",
    "1. learning rate\n",
    "2. batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9226993865030675\n",
      "2 0.8785276073619632\n",
      "3 0.8846625766871166\n",
      "4 0.8907975460122699\n",
      "5 0.905521472392638\n",
      "6 0.9042944785276074\n",
      "7 0.905521472392638\n",
      "8 0.905521472392638\n",
      "9 0.905521472392638\n",
      "10 0.9042944785276074\n",
      "11 0.9042944785276074\n",
      "12 0.9042944785276074\n",
      "13 0.9325153374233128\n",
      "14 0.9276073619631902\n",
      "15 0.9337423312883436\n",
      "16 0.9251533742331288\n",
      "17 0.9300613496932515\n",
      "18 0.9300613496932515\n",
      "19 0.9337423312883436\n",
      "20 0.9337423312883436\n",
      "21 0.9300613496932515\n",
      "22 0.9300613496932515\n",
      "23 0.9312883435582822\n",
      "24 0.9337423312883436\n",
      "25 0.9349693251533743\n",
      "26 0.9300613496932515\n",
      "27 0.9300613496932515\n",
      "28 0.9349693251533743\n",
      "29 0.9337423312883436\n",
      "30 0.9276073619631902\n",
      "31 0.9288343558282208\n",
      "32 0.9349693251533743\n",
      "33 0.9276073619631902\n",
      "34 0.9349693251533743\n",
      "35 0.9214723926380368\n",
      "36 0.9411042944785276\n",
      "37 0.9582822085889571\n",
      "38 0.7975460122699386\n",
      "39 0.9239263803680982\n",
      "40 0.9239263803680982\n",
      "41 0.9239263803680982\n",
      "42 0.9239263803680982\n",
      "43 0.9226993865030675\n",
      "44 0.9239263803680982\n",
      "45 0.9239263803680982\n",
      "46 0.9239263803680982\n",
      "47 0.9239263803680982\n",
      "48 0.9239263803680982\n",
      "49 0.9239263803680982\n",
      "50 0.9239263803680982\n",
      "51 0.9239263803680982\n",
      "52 0.9239263803680982\n",
      "53 0.9239263803680982\n",
      "54 0.9239263803680982\n",
      "55 0.9239263803680982\n",
      "56 0.9239263803680982\n",
      "57 0.9239263803680982\n",
      "58 0.9239263803680982\n",
      "59 0.9239263803680982\n",
      "60 0.9239263803680982\n",
      "61 0.9239263803680982\n",
      "62 0.9239263803680982\n",
      "63 0.9239263803680982\n",
      "64 0.9239263803680982\n",
      "65 0.9239263803680982\n",
      "66 0.9239263803680982\n",
      "67 0.9239263803680982\n",
      "68 0.9239263803680982\n",
      "69 0.9239263803680982\n",
      "70 0.9239263803680982\n",
      "71 0.9239263803680982\n",
      "72 0.9239263803680982\n",
      "73 0.9239263803680982\n",
      "74 0.9239263803680982\n",
      "75 0.9239263803680982\n",
      "76 0.9239263803680982\n",
      "77 0.9239263803680982\n",
      "78 0.9239263803680982\n",
      "79 0.9239263803680982\n",
      "80 0.9239263803680982\n",
      "81 0.9239263803680982\n",
      "82 0.9239263803680982\n",
      "83 0.9239263803680982\n",
      "84 0.9239263803680982\n",
      "85 0.9239263803680982\n",
      "86 0.9239263803680982\n",
      "87 0.9239263803680982\n",
      "88 0.9239263803680982\n",
      "89 0.9239263803680982\n",
      "90 0.9239263803680982\n",
      "91 0.9239263803680982\n",
      "92 0.9239263803680982\n",
      "93 0.9239263803680982\n",
      "94 0.9239263803680982\n",
      "95 0.9239263803680982\n",
      "96 0.9239263803680982\n",
      "97 0.9239263803680982\n",
      "98 0.9239263803680982\n",
      "99 0.9239263803680982\n",
      "100 0.9239263803680982\n"
     ]
    }
   ],
   "source": [
    "### Part 2.C\n",
    "\n",
    "mlp = MLP(batch_size=10, lr=0.0005)\n",
    "acc = []\n",
    "epochs = [i+1 for i in range(100)]\n",
    "for i in epochs:\n",
    "    mlp.train(X_train.T[1:].T, y_train, i)\n",
    "    acc.append(mlp.test(X_test.T[1:].T, y_test))\n",
    "    print (i, acc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1187f4e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, acc, label='MLP')\n",
    "plt.plot(epochs, [baseline_score]*len(epochs), label='Baseline')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Batch-Size=10, Learning Rate=0.0005')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 2.D**\n",
    "\n",
    "Now train nets with varying size of the hidden layer $H={1, 2, 5, 10..}$ for max epochs = 100. Make a plot of the nets' accuracy on test set as a function of $H$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7926380368098159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.7926380368098159\n",
      "5 0.9153374233128835\n",
      "10 0.9165644171779141\n",
      "15 0.9153374233128835\n",
      "20 0.9214723926380368\n",
      "25 0.90920245398773\n",
      "35 0.9214723926380368\n",
      "50 0.9251533742331288\n"
     ]
    }
   ],
   "source": [
    "### Part 2.D\n",
    "H = [1, 2, 5, 10, 15, 20, 25, 35, 50]\n",
    "acc_h = []\n",
    "for h in H:\n",
    "    mlp_h = MLP(n_hid=h, batch_size=10, lr=0.0005)\n",
    "    mlp_h.train(X_train.T[1:].T, y_train, 100)\n",
    "    acc_h.append(mlp_h.test(X_test.T[1:].T, y_test))\n",
    "    print (h, acc_h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8XXWd//HXO0nTfYMmFbpQCqULoiAVmAGFFkFABMdlBEXBDXVE5+fAzx8oIjLiqD9ndFTEHyqCK3Zc64Aiw+YG2iICdoOCLKGQpLQ06ZK0ST6/P8657SEkuTclJ8u97+fjcR+5Z7v3c+69OZ9zvt/z/X4VEZiZmfWlaqgDMDOz4c/JwszMinKyMDOzopwszMysKCcLMzMrysnCzMyKcrIwACQ9KulVA/Rab5X064F4LQNJH5X0jaGOwyqbk8Uwlh7Ad0jaKmmzpBslzSpx2zmSQlJNTrEdJ+kPkrZI2iTp95JeDhAR34uIk/N43x7i2E/Sckkb0v2d0235aEnXSmqR9LSkf+nHa18n6VMDHXN/RcSnI+Ldebx2+pltS39jT0r6D0nVJW57gqSGnOI6XNI9kranfw/vY919JP003Y/HJL2l2/K3pPO3SfqZpH1K2Tbdv670syk8zs1jf0cCJ4vh77URMQHYD2gEvjzE8SBpEvDfaSz7ADOATwLtQxBOF/Ar4A29LL8cmAccACwBPiLplMEJrbi8knk/vTT9jR0PvBl451AGI6kW+DnwXWAqcD3w83R+T64CdgLTgbcCV0s6NH2tQ4H/B7wtXb4d+Gop26Y2RMSEzOP6AdrNkSci/BimD+BR4FWZ6dOABzPTrwHuBVqAJ4DLM8seBwLYmj7+Lp3/HmAN0AqsBl6Wea+LgPuBLcAPgTG9xLUYeLaPuM8Dfpc+/0gmhq3ALuC6dNlk4JvAU8CTwKeA6r38rGrS/Z3Tbf6TwMmZ6X8FbijxNa8DPtXLsgXALcAmYB3wjyV+L3PSON+Vfke/ycw7N523EfhYZpvLge922763dceSHFw3p9/zR4CGPvYxgIMz08uAqzLT78j8Xh4B3pvOHw/sIEnWhe92f5IT0IuBh4Fn0tfbp5/f5cnp96Zuv+dTelh3PMnB/pDMvO8An0mffxr4fmbZQen6E0vY9oS+PrtKe/jKYoSQNI7krO/uzOxtwNuBKSQHqPdLel267JXp3ymRnBHdJelNJAeetwOTgDNI/qEL/hE4BTgQeAnJQb8nDwKdkq6XdKqkqb3FHRGfS99/ArAQaCY5gEByUOsADgaOIDlIvDvd3+MkPdvH47jeP61EGtf+wH2Z2fcBh/a8RWkkjSdJFN8H6oGzga9mzkj7+l4Kjif5PF6dmXccMB84EbhM0sI+wuht3U+QJJS5wEnAOf3YrwXAK4D1mdlNwOkkv5d3AF+Q9LKI2AacynPPvDcAHwJel+7f/iRJ66rMe/T1nV6crnYocH+kR+zU/fT8vR0CdEbEg5l52e/4UDLff0Q8TJogStgWoF5So6S/SfpC+t1XJCeL4e9nkp4lOUs9Cfi/hQURcUdEPBARXRFxP/ADkn/S3rwb+FxErIjE+oh4LLP8SxGxISI2Ab8AeiwnjogWkoNVAF8HmtN6g+m9vbGkscDPgP+MiJvSdU8F/ldEbIuIJuALwFnpe/wuIqb08fhd3x8bABPSv1sy87aQnFW+EKcDj0bEtyKiIyL+DPwYeGMaeynfy+Xpfu/IzPtkROyIiPtIDlov7SOG3tb9R+DTEbE5IhqAL5WwP3+WtI3kCuIOMsU0EXFjRDyc/l7uBH5NklB6816SK52GiGgnOTl5Y6G4rch3+pn0NSbw3O8Mev/eiq3b1/Ji264l+R/YD1gKHAn8R++7Xt6cLIa/10XEFGA0cAFwp6QXAUg6WtLtkpolbQHeB0zr47VmkRQP9ObpzPPtpAdbSb/MVPC9FSAi1kTEeRExE3gxyVnkF/t47W8C6yLis+n0AcAo4KnCmSVJ2XJ9H6/RX1vTv5My8yaRFKm8EAcAR2fPiknKu/vzvTzRw+v2+Pn3ord19+/22j29T3cvS7d/M3A0SfEMAOmV493pTQzPkhSF9vUbOwD4aeZzWQN0ktQJlGorz/3OoPfvrdi6fS3vc9uIeDoiVqdJ/28kRXpv7Md+lBUnixEiIjoj4ick/3iFIpjvA8uBWRExGfgaoMImPbzMEyRltv1971MzRQ3f62H5WpLy/Rf3tH1avDCfpJw+G0s7MC1zZjkpIgoVk6/odhdK90dfZ7eFuDaT1Idkz9BfCqwqbc979QRwZ7ez4gkR8f50eV/fy+7wXmAMvXkKmJmZLunuufTKYRlwF3AZJHeSkVwxfR6Ynp603ETx39ip3T6bMRHxZPqafX2nH01fYxXwEknZz+wl9Py9PQjUSJqXmZf9jleR+f4lzSU58XqwhG2f9zHx/O+xYjhZjBBKnElyd8iadPZEYFNEtEk6CsjeMthMUvk4NzPvG8BFko5MX+9gSQfsRSwLJF0oaWY6PYuk3P7uHtY9lbQcO1vkEhFPkRRp/LukSZKqJB0k6fh0+W/juXehdH/8NvMeY0gOAACj0+mCbwOXSpqalsm/hySxFbYNSSf0sbvVksZkHrUkd4IdIultkkalj5dn6g36+l7ytgy4JN3fGSRXo/3xGeD89Oq1luRzbQY60u8ye0t0I7CvpMmZeV8Driz8riTVpb9bAIp8p59OV7uD5KToQ0pufS7sw23dg03rTn4CXCFpvKRjgTNJKqoBvge8Nj35GA9cAfwkIlqLbavk1tnZ6f/KrPSz+Xk/P8+y4WQx/P1C0laSOosrgXMjonDm808kP/RWkrPBQsUxEbE9Xf/3aZHAMRHxX+m875Ncav+M5NbX/molKa74Y1rWfTfwV+DCHtZ9M1AHrMmcQX4tXfZ2kgPSapKK0B+RlA/31w72FDmtTacLPkFS9PYYcCfwfyPiVwBpstsKPNDHa1+cvl7hcVtEtJIcNM8CNpAUCX2WPQmr1+9lEFwBNAB/A/6H5DMt+ZbmiHiA5HP63+l+fogk/s0kSW95Zt21JPUxj6S/sf2B/0zX+XW6/3eT/FZKFhE7SSrJ3w48S3Ir7+vS+YVGir/MbPJPJHeBNaXxvL/wP5L+fR9J0mgiSeT/VMq2JMVzd5HcsPAHkt/4h/qzL+VEEXldDZsNb5LOAQ6NiEuGOpa8SHo/cFZE9HXjg1lRThZmZUTSfiRFj3eRNEa8EfhKRPR184FZUcOh9aiZDZxakrvKDiQpwrmB57ZYNtsrvrIwM7OiXMFtZmZFlU0x1LRp02LOnDlDHYaZ2Yhyzz33bIyIumLrlU2ymDNnDitXrhzqMMzMRhRJjxVfy8VQZmZWAicLMzMrysnCzMyKcrIwM7OinCzMzKwoJwszMyvKycLMzIoqm3YWZmblqKOzi63tHbS2ddDStovWto70sWv336nja3nr0f0emqZfnCzMzHLS2RVsfc5Bfhct3Q70SRJ4/rxCcti+s7Po+xwxe4qThZnZUOjpQN/a1kFr+3MP5i07ej7Qt7btYlsJB/ramiomjalh4phRTBxTw8QxNdRPHJM+T+ZNGpv+fc56e9YfXVOd++fhZGFmZaezK9Kim/TMfUfPB/qeinQK65d0oK+uYtLYvg/0yUF+VI8H/8E60A8EJwszG1a6uoLW9uefpT+3GKeHM/7M+lvbO4q+T2111e4DduEgXjdtQtEDffb5mFEj40A/EJwszGzAdHUFW3fuOcB3L6Jp6eVMPnvw39sD/bRp459TRDPJB/oBlWuykHQKyQDu1cA3IuIz3ZYfAFwL1AGbgHMiokHS4cDVwCSgE7gyIn6YZ6xWHjY8u4Pb1jZx+9om/vToJo4/pI6PnraQ/aeMHerQytbmbTu588FmblvbxJ0PNrNlx64+1x9Vrd0H88KBe860cc870D/3IP/c9X2gH3y5JQtJ1cBVwElAA7BC0vKIWJ1Z7fPAtyPieklLgX8D3gZsB94eEQ9J2h+4R9LNEfFsXvHayNTZFfzlic3cuqaJ29Y2sfbpVgBm7TOWJfPruXnV09y6pokPLDmId79irg8yAyAiWNfYyq1rkqT858c30xUwbUItJy2azoIXTXzegT5bpDO6pgpJQ70b1k95XlkcBayPiEcAJN0AnAlkk8Ui4MPp89uBnwFExIOFFSJig6QmkqsPJwtjy/Zd3PlQM7evbeKOdU1s3r6L6iqx+ICpfPS0BSxdUM9BdROQxBObtnPljWv4/K8f5L/uaeDjr1nEiQvrfbDqp7Zdnfzh4Y3ctraJ29Y0sWFLGwCHzZjMBUvnceKCeg6bMZmqKn+u5SrPZDEDeCIz3QAc3W2d+4A3kBRV/QMwUdK+EfFMYQVJR5EMQv9w9zeQdD5wPsDs2bMHNHgbPiKC9U1buXVtcvVwz2Ob6ewKpo4bxZL59SxZUM8rD6lj8thRz9t21j7j+NrbjuR3D23k8l+s4t3fXskJ8+u47PRFzK2bMAR7M3IUivRuW9vEHx7eSNuuLsbVVnPcwdP451fNY8n8euonjRnqMG2QKCLyeWHpTcCrI+Ld6fTbgKMi4oOZdfYHvgIcCPyGJHEcGhFb0uX7AXcA50bE3X293+LFi8Mj5ZWPtl2d3P3IM9y+tolb1zbRsHkHAAv3m8SJC5IEcfisKVT340x2V2cX1//hUf7zfx6iraOTdx03lwuWHsyE0b7PA5IivXsf37w7QRSK9GbvM46lC+pZuqCeo+fuM2Ju9bTSSLonIhYXWy/P/5IGYFZmeiawIbtCRGwAXg8gaQLwhkyimATcCFxaLFFYeXh6S9vuA9Xv129kx65Oxoyq4riDp/H+Ew5iyfz6F1RRPaq6ine/Yi5nHL4/n/vVOr5258P89N4GLjl1IWcevn9FFk1t2b6LOx5M6h7ufLCZzdt3UVMlFs+ZysdOW8iSBfUcVDe+Ij8be648ryxqgAeBE4EngRXAWyJiVWadacCmiOiSdCXQGRGXSaoFfgn8IiK+WMr7+cpi5OnsCu5reJbb0srp1U+1ADBjylhOXJhcPfzd3H1zq5T+8+ObuXz5Ku5v2MLL50zl8jMO5dD9J+fyXsNFRPBQ09bddQ/3PJ4U6e0zvpYT5texdEE9r5jXc5GeladSryxySxZpEKcBXyS5dfbaiLhS0hXAyohYLumNJHdABUkx1Aciol3SOcC3gFWZlzsvIv7S23tVUrKICLbv7Oylr5nn38O+tb2DsbXVmVsQ+25wNL62JreKyi07dvHbh5LbLO9Y18ymbTuprhJHzp7KkgX1nLiwnnn1EwbtTLarK1i28gk+d/M6nt2+k7ccPZsLT5rP1PG1g/L+g6FQpFe4aisU6S3ab9LupPzSmf0r0rPyMSySxWAaKcmivwf6lh5aqm5t76Czq+/vrUrsTgDja2vYsatz9/YdRbaVYMLomuckle73uRdr4VpIOBHBw83b0rqHRlY+upmOrmDKuFGccEgdSxbUc/whdUwZN7QH5y3bd/GF/3mQ79z9GBPH1HDRyfM5+6jZI/YAuqdIr5Hfr3+GHbs6GTuqmmMPnsbSBfUsWVDHfpPd9sScLIaVxpY23vbNP9LY0t7vA/3uvmR6aYWaXZ4ctJPpcbXVPZ6dRwRtu7p6TFRJUuqp75znJ7NSE05tdRXPbNsJwIIXTUyuHhbUc8TsqcPyQLz26RYuX76Kux/ZxKL9JvHJMw/l5XP2GeqwiuqtSG/m1LG7K6ePybFIz0YuJ4th5JbVjbzn2ys546X7M2ufsUUSwSjG93KgHy76SjjJvD3T23d2cNjMKSxdUM+MEdKKOiK48YGnuPLGNTy1pY3XHb4/l5y2kOnD7DbR3UV6a5q448FMkd4BU3cniMEs0rORaTjcDWWpxpakAdPHXjP8Djh7QxJja6sZW1tN/aShjmbgSeL0l+zP0gX1fPX2h7nmN49wy+pGPnjiPN557IHU1gzNAJNJkd7W3XUPKx5NKqcLRXpLF07n+Hl1TB7nymkbeE4Wg6CppY0qwb5lVGlaCcbV1nDRq+fzpsUz+df/Xs1nfrmWZSue4LLXLuKE+fWDEkN7Ryd/fGTT7gTx+KbtQFKk995XzuXEhfUcPmt4FulZeXGyGASNLe1MmzCammoPeT4SHbDveL5x7su5fV0TV/xiNed9awWvWjidy05fxOx9xw34+zW2tHF7mhx+t34j23d2MrqmimMPnsb5r5zLkhFUpGflw8liEDS2tpVF8VOlWzK/nmMPmsa1v/8bX771IV71hTt57yvn8v4TDmJc7d7/K3V1Bfc/uYXb1jRy27om/vrknvYmr3/ZDJYuqOfv5k5jbK0rp23oOFkMgsaWdmZMcbIoB7U1Vbzv+IP4hyNm8G83reHLt63nx/c08NHXLOQ1h+1XcmVya9sufvvQxrS9SRMbt+6kSvCy2VP5yCnzWbqgnvnTJ7py2oYNJ4tB0NTSxhGzpwx1GDaApk8awxfPOoK3HnMAn/j5Ki74/r18b+7jXH7Gocx/0cQet3kkUzn9p79toqMrmDSmhhPmJ3cuHX9IXVk1BrTy4mSRs50dXTyzbSfTJ/rKohy9fM4+/OKDx/GDPz3O53+9jtO+9FvedswBfPikQxg7qpo//W3T7sZxjz6TVE4fMn0C737FXJYuqOdls6e4LstGBCeLnDVvbQdg+qTRQxyJ5aW6SpxzzAG85rD9+Pdb1vHtux7lZ395ko7OYGt7B7U1Vfz9QfvyzuMOZMn8embtM/CV4mZ5c7LIWaGNxfTJvrIod1PH1/Kp1x3G2UfN5iu3rWfKuFpOXFDP3x+87wuqADcbDvwLzllTIVm4GKpiHLr/ZK4+58ihDsNsQLmwNGeNLS6GMrORz8kiZ40tbYyqFlOHuFdVM7MXwskiZ40t7dRPHOOB7M1sRHOyyFlTaxv1LoIysxHOySJnT29pc+W2mY14ThY5a2xpc+W2mY14ThY52rGzk5a2DurdiaCZjXBOFjlqak3bWDhZmNkI52SRI7exMLNykWuykHSKpHWS1ku6uIflB0i6VdL9ku6QNDOz7FxJD6WPc/OMMy+7u/rwlYWZjXC5JQtJ1cBVwKnAIuBsSYu6rfZ54NsR8RLgCuDf0m33AT4BHA0cBXxC0tS8Ys1Lo7v6MLMykeeVxVHA+oh4JCJ2AjcAZ3ZbZxFwa/r89szyVwO3RMSmiNgM3AKckmOsuWhqbWd0TRWTxroLLjMb2fJMFjOAJzLTDem8rPuAN6TP/wGYKGnfErdF0vmSVkpa2dzcPGCBD5TkttkxHu3MzEa8PJNFT0fI6DZ9EXC8pHuB44EngY4StyUiromIxRGxuK6u7oXGO+DcxsLMykWeyaIBmJWZnglsyK4QERsi4vURcQTwsXTellK2HQmaWtpduW1mZSHPZLECmCfpQEm1wFnA8uwKkqZJKsRwCXBt+vxm4GRJU9OK7ZPTeSNKoRjKzGykyy1ZREQHcAHJQX4NsCwiVkm6QtIZ6WonAOskPQhMB65Mt90E/CtJwlkBXJHOGzG2tnewbWeni6HMrCzkeptORNwE3NRt3mWZ5z8CftTLttey50pjxHEbCzMrJ27BnZNCsqh3GwszKwNOFjlpclcfZlZGnCxy8nThysLFUGZWBpwsctLY0saE0TVMGO3W22Y28jlZ5KSppd3DqZpZ2XCyyElji4dTNbPy4WSRk8ZWd/VhZuXDySIHEUGju/owszLiZJGDLTt2sbOjy3dCmVnZcLLIgYdTNbNy42SRA3f1YWblxskiBx5O1czKjZNFDppak2Iot7Mws3LhZJGDxpY2powbxZhR1UMdipnZgHCyyIEb5JlZuXGyyEGju/owszLjZJGDJg+namZlxsligHV1BU2t7W5jYWZlxcligD2zbScdXeErCzMrK04WA8zDqZpZOco1WUg6RdI6SeslXdzD8tmSbpd0r6T7JZ2Wzh8l6XpJD0haI+mSPOMcSE2thdbbLoYys/KRW7KQVA1cBZwKLALOlrSo22qXAssi4gjgLOCr6fw3AaMj4jDgSOC9kubkFetA2tMvlK8szKx85HllcRSwPiIeiYidwA3Amd3WCWBS+nwysCEzf7ykGmAssBNoyTHWAVMohqqb6CsLMysfeSaLGcATmemGdF7W5cA5khqAm4APpvN/BGwDngIeBz4fEZtyjHXANLa0M21CLaOqXR1kZuUjzyOaepgX3abPBq6LiJnAacB3JFWRXJV0AvsDBwIXSpr7vDeQzpe0UtLK5ubmgY1+LzW1tLly28zKTp7JogGYlZmeyZ5ipoJ3AcsAIuIuYAwwDXgL8KuI2BURTcDvgcXd3yAiromIxRGxuK6uLodd6D8Pp2pm5SjPZLECmCfpQEm1JBXYy7ut8zhwIoCkhSTJojmdv1SJ8cAxwNocYx0wHk7VzMpRbskiIjqAC4CbgTUkdz2tknSFpDPS1S4E3iPpPuAHwHkRESR3UU0A/kqSdL4VEffnFetA6ejsYuNWJwszKz81eb54RNxEUnGdnXdZ5vlq4NgetttKcvvsiLJx604ifNusmZUf37IzgPYMp+o6CzMrL04WA8hjb5tZuXKyGECNHk7VzMqUk8UAamppo7pK7DveycLMyouTxQB6eksbdRNGU13VU3tEM7ORy8liADV60CMzK1NFk4WkCyRNHYxgRrqmljbqXbltZmWolCuLFwErJC1Lx6dwGUsvGlvc1YeZlaeiySIiLgXmAd8EzgMekvRpSQflHNuI0t7Ryebtu5juTgTNrAyVVGeRdsHxdProAKYCP5L0uRxjG1GaPOiRmZWxot19SPoQcC6wEfgG8L8jYlfalfhDwEfyDXFkKAyn6jYWZlaOSukbahrw+oh4LDszIroknZ5PWCOPh1M1s3JWSjHUTcDuUeokTZR0NEBErMkrsJHGXX2YWTkrJVlcDWzNTG9L51lGY0s7tdVVTB03aqhDMTMbcKUkC6UV3EBS/ETOXZuPREkbi9H4zmIzK0elJItHJH1I0qj08c/AI3kHNtIkw6m6CMrMylMpyeJ9wN8DT5KMq300cH6eQY1EyXCqvhPKzMpT0eKkiGgiGT/b+tDY0sZxB08b6jDMzHJRSjuLMcC7gEOB3eUsEfHOHOMaUbbv7KC1rcPFUGZWtkophvoOSf9QrwbuBGYCrXkGNdLsab3tYigzK0+lJIuDI+LjwLaIuB54DXBYvmGNLE+7jYWZlblSksWu9O+zkl4MTAbmlPLiaS+16yStl3RxD8tnS7pd0r2S7pd0WmbZSyTdJWmVpAfS4rBhaU+DPF9ZmFl5KqW9xDXpeBaXAsuBCcDHi20kqRq4CjiJ5C6qFZKWR8TqzGqXAssi4mpJi0hai8+RVAN8F3hbRNwnaV/2JK1hp1AM5bEszKxc9Zks0s4CWyJiM/AbYG4/XvsoYH1EPJK+1g3AmUA2WQQwKX0+GdiQPj8ZuD8i7gOIiGf68b6DrrGljbGjqpk42m0Vzaw89VkMlbbWvmAvX3sG8ERmuiGdl3U5cI6kBpKrig+m8w8BQtLNkv4sqceebSWdL2mlpJXNzc17GeYLVxhO1a23zaxclVJncYukiyTNkrRP4VHCdj0dOaPb9NnAdRExEzgN+E56NVMDHAe8Nf37D5JOfN6LRVwTEYsjYnFdXV0JIeWj0cOpmlmZK6XcpNCe4gOZeUHxIqkGYFZmeiZ7ipkK3gWcAhARd6WV2NPSbe+MiI0Akm4CXgbcWkK8g66ppY3DZk4Z6jDMzHJTyrCqB/bwKKXuYgUwT9KBkmpJWoEv77bO48CJAJIWkjT6awZuBl4iaVxa2X08z63rGDYiIunqY6LvhDKz8lVKC+639zQ/Ir7d13YR0SHpApIDfzVwbUSsknQFsDIilgMXAl+X9GGSq5Xz0h5uN0v6D5KEE8BNEXFjf3ZssLS2d7BjV6fbWJhZWSulGOrlmedjSK4E/gz0mSwAIuImkorr7LzLMs9XA8f2su13SW6fHdaaCm0sJjtZmFn5KqUjwQ9mpyVNJukCxMgMp+piKDMrY6XcDdXddmDeQAcyUnk4VTOrBKXUWfyCPbe8VgGLgGV5BjWSNO5uve0rCzMrX6XUWXw+87wDeCwiGnKKZ8RpbGlj4pgaxtW69baZla9SjnCPA09FRBuApLGS5kTEo7lGNkI0eThVM6sApdRZ/BfQlZnuTOcZHk7VzCpDKcmiJiJ2FibS57X5hTSyPL2ljekTfWVhZuWtlGTRLOmMwoSkM4GN+YU0ckQETa3uF8rMyl8pdRbvA74n6SvpdAPQY6vuSrN5+y52dYaLocys7JXSKO9h4BhJEwBFhMffTrmNhZlViqLFUJI+LWlKRGyNiFZJUyV9ajCCG+48nKqZVYpS6ixOjYhnCxPpqHmn9bF+xdg9nKoruM2szJWSLKol7T51ljQW8Kk0e64s3HrbzMpdKRXc3wVulfStdPodwPX5hTRyNLa2MXXcKEbXVA91KGZmuSqlgvtzku4HXkUyVOqvgAPyDmwkSBrkuQjKzMpfqb3OPk3SivsNJONZrMktohGkqcVdfZhZZej1ykLSISRDoZ4NPAP8kOTW2SWDFNuw19jSzvwXTRzqMMzMctdXMdRa4LfAayNiPUA6/KkBnV1B81YXQ5lZZeirGOoNJMVPt0v6uqQTSeosDHhmWzudXeGuPsysIvSaLCLipxHxZmABcAfwYWC6pKslnTxI8Q1bTR5O1cwqSNEK7ojYFhHfi4jTgZnAX4CLc49smHNXH2ZWSfo1BndEbIqI/xcRS0tZX9IpktZJWi/peQlG0mxJt0u6V9L9kk7rYflWSRf1J87B8LSThZlVkH4li/6QVA1cBZxKMm732ZIWdVvtUmBZRBxBcufVV7st/wLwy7xifCEaW9qRYNoED+1hZuUvt2QBHAWsj4hH0gGTbgDO7LZOAJPS55OBDYUFkl4HPAKsyjHGvdbU0sa0CaOpqc7zIzQzGx7yPNLNAJ7ITDek87IuB86R1ADcBHwQQNJ44P8An+zrDSSdL2mlpJXNzc0DFXdJGlva3NusmVWMPJNFT7fZRrfps4HrImImSU+235FURZIkvhARW/t6g4i4JiIWR8Tiurq6AQm6VI0t7R5O1cyfADNHAAAOd0lEQVQqRikdCe6tBmBWZnommWKm1LuAUwAi4i5JY4BpwNHAGyV9DpgCdElqi4ivMEw0tbbx0llThjoMM7NBkWeyWAHMk3Qg8CRJBfZbuq3zOElfU9dJWgiMAZoj4hWFFSRdDmwdToliV2cXG7fudDGUmVWM3IqhIqIDuAC4maTjwWURsUrSFZLOSFe7EHiPpPuAHwDnRUT3oqphp7k1bZDn22bNrELkeWVBRNxEUnGdnXdZ5vlq4Ngir3F5LsG9AB5O1cwqje/73AuNLb6yMLPK4mSxF5pa3XrbzCqLk8VeaGxpo6ZK7DPOrbfNrDI4WeyFxpZ26ieOpqrKPbabWWVwstgLjS1tHsfCzCqKk8VeaGpp951QZlZRnCz2QmNrmyu3zayiOFn0U9uuTp7dvsvJwswqipNFPxWGU633cKpmVkGcLPqp0W0szKwCOVn0k8feNrNK5GTRT3u6+nAxlJlVDieLfmpqaaO2porJY0cNdShmZoPGyaKfCsOpSm69bWaVw8minzycqplVIieLfnKDPDOrRE4W/ZR09eFkYWaVxcmiH7a2d7C1vcN3QplZxXGy6Icmt7EwswrlZNEPhTYW9b6yMLMK42TRDx5O1cwqVa7JQtIpktZJWi/p4h6Wz5Z0u6R7Jd0v6bR0/kmS7pH0QPp3aZ5xlspdfZhZparJ64UlVQNXAScBDcAKScsjYnVmtUuBZRFxtaRFwE3AHGAj8NqI2CDpxcDNwIy8Yi3V01vaGV9bzYTRuX1sZmbDUp5XFkcB6yPikYjYCdwAnNltnQAmpc8nAxsAIuLeiNiQzl8FjJE05BUFbmNhZpUqz2QxA3giM93A868OLgfOkdRAclXxwR5e5w3AvRHR3n2BpPMlrZS0srm5eWCi7kNTS5srt82sIuWZLHrqPCm6TZ8NXBcRM4HTgO9I2h2TpEOBzwLv7ekNIuKaiFgcEYvr6uoGKOzeNbpBnplVqDyTRQMwKzM9k7SYKeNdwDKAiLgLGANMA5A0E/gp8PaIeDjHOEsSEWkngk4WZlZ58kwWK4B5kg6UVAucBSzvts7jwIkAkhaSJItmSVOAG4FLIuL3OcZYspYdHbR3dHk4VTOrSLkli4joAC4guZNpDcldT6skXSHpjHS1C4H3SLoP+AFwXkREut3BwMcl/SV91OcVayk8nKqZVbJc7wGNiJtIKq6z8y7LPF8NHNvDdp8CPpVnbP3lNhZmVsncgrtEHk7VzCqZk0WJfGVhZpXMyaJETS1tTB47ijGjqoc6FDOzQedkUaKkjYWLoMysMjlZlMhdfZhZJXOyKFFTSzv1E50szKwyOVmUoKsraGptczGUmVUsJ4sSbN6+k12d4WIoM6tYThYleHr3bbO+sjCzyuRkUYKm3WNv+8rCzCqTk0UJ3CDPzCqdk0UJCl191E1wMZSZVSYnixI0trax7/haamv8cZlZZfLRrwTJcKougjKzyuVkUQJ39WFmlc7JogSNLW1Md+ttM6tgThZFdHR2sXGrryzMrLI5WRTxzLaddAVMn+wrCzOrXE4WRexuY+FiKDOrYE4WRewZTtXJwswql5NFEY3uF8rMLN9kIekUSeskrZd0cQ/LZ0u6XdK9ku6XdFpm2SXpduskvTrPOPvS1NJGlWBft942swpWk9cLS6oGrgJOAhqAFZKWR8TqzGqXAssi4mpJi4CbgDnp87OAQ4H9gf+RdEhEdOYVb28aW9qpmzia6ioN9lubmQ0beV5ZHAWsj4hHImIncANwZrd1ApiUPp8MbEifnwncEBHtEfE3YH36eoPOw6mameWbLGYAT2SmG9J5WZcD50hqILmq+GA/tkXS+ZJWSlrZ3Nw8UHE/x9Nb2jycqplVvDyTRU/lNtFt+mzguoiYCZwGfEdSVYnbEhHXRMTiiFhcV1f3ggPuSVOrG+SZmeVWZ0FyNTArMz2TPcVMBe8CTgGIiLskjQGmlbht7to7Otm0baeLocys4uV5ZbECmCfpQEm1JBXWy7ut8zhwIoCkhcAYoDld7yxJoyUdCMwD/pRjrD1qbi20sfCVhZlVttyuLCKiQ9IFwM1ANXBtRKySdAWwMiKWAxcCX5f0YZJipvMiIoBVkpYBq4EO4ANDdScUeDhVM7M8i6GIiJtIKq6z8y7LPF8NHNvLtlcCV+YZXzFN7urDzAxwC+4+ufW2mVnCyaIPja3tjKoWU8fVDnUoZmZDysmiD40tSRuLKrfeNrMK52TRhyYPp2pmBuRcwT0SPLt9J2/62l09Lnt803aWzK8f5IjMzIafik8WVVVi3vQJPS6bN30CbznqgEGOyMxs+Kn4ZDFpzCi++tYjhzoMM7NhzXUWZmZWlJOFmZkV5WRhZmZFOVmYmVlRThZmZlaUk4WZmRXlZGFmZkU5WZiZWVFKxhoa+SQ1A48VWW0asHEQwhmOKnXfvd+VxfvdfwdERF2xlcomWZRC0sqIWDzUcQyFSt1373dl8X7nx8VQZmZWlJOFmZkVVWnJ4pqhDmAIVeq+e78ri/c7JxVVZ2FmZnun0q4szMxsLzhZmJlZURWTLCSdImmdpPWSLh7qePIi6VpJTZL+mpm3j6RbJD2U/p06lDHmQdIsSbdLWiNplaR/TueX9b5LGiPpT5LuS/f7k+n8AyX9Md3vH0qqHepY8yCpWtK9kv47na6U/X5U0gOS/iJpZTov1996RSQLSdXAVcCpwCLgbEmLhjaq3FwHnNJt3sXArRExD7g1nS43HcCFEbEQOAb4QPodl/u+twNLI+KlwOHAKZKOAT4LfCHd783Au4Ywxjz9M7AmM10p+w2wJCIOz7SvyPW3XhHJAjgKWB8Rj0TETuAG4MwhjikXEfEbYFO32WcC16fPrwdeN6hBDYKIeCoi/pw+byU5gMygzPc9ElvTyVHpI4ClwI/S+WW33wCSZgKvAb6RTosK2O8+5Ppbr5RkMQN4IjPdkM6rFNMj4ilIDqpA/RDHkytJc4AjgD9SAfueFsX8BWgCbgEeBp6NiI50lXL9vX8R+AjQlU7vS2XsNyQnBL+WdI+k89N5uf7WawbyxYYx9TDP9wyXIUkTgB8D/ysiWpKTzfIWEZ3A4ZKmAD8FFva02uBGlS9JpwNNEXGPpBMKs3tYtaz2O+PYiNggqR64RdLavN+wUq4sGoBZmemZwIYhimUoNEraDyD92zTE8eRC0iiSRPG9iPhJOrsi9h0gIp4F7iCps5kiqXAyWI6/92OBMyQ9SlKsvJTkSqPc9xuAiNiQ/m0iOUE4ipx/65WSLFYA89I7JWqBs4DlQxzTYFoOnJs+Pxf4+RDGkou0vPqbwJqI+I/MorLed0l16RUFksYCryKpr7kdeGO6Wtntd0RcEhEzI2IOyf/zbRHxVsp8vwEkjZc0sfAcOBn4Kzn/1iumBbek00jOPKqBayPiyiEOKReSfgCcQNJlcSPwCeBnwDJgNvA48KaI6F4JPqJJOg74LfAAe8qwP0pSb1G2+y7pJSSVmdUkJ3/LIuIKSXNJzrj3Ae4FzomI9qGLND9pMdRFEXF6Jex3uo8/TSdrgO9HxJWS9iXH33rFJAszM9t7lVIMZWZmL4CThZmZFeVkYWZmRTlZmJlZUU4WZmZWlJOFDRuStnabPk/SV9Ln75P09h62mZPtYbfbsjskveBB7CWdUOjVdLCln0FXeotsYd5f0y5NzAaNk4WNCBHxtYj49lDHkbdM6+OsBuBjg/ReZj1ysrARQdLlki5Knx+Zjt9wF/CBzDpjJd0g6X5JPwTGZpadLOkuSX+W9F9pH1KFcQE+mc5/QNKCfsR0maQV6Zn+NUocJOnPmXXmSbonE/edaedvN2e6ZrhD0qcl3UnS5XZ3/w0cKml+DzH0tV/T0ueLJd2R+RyvkfRr4NtKxsP4Vrrv90pakq53nqSfSPqVkvERPpfOr5Z0XbrPD0j6cKmfl41sThY2nIxVMpjLX5T0onpFL+t9C/hQRPxdt/nvB7ZHxEuAK4EjAdKD5qXAqyLiZcBK4F8y221M518NXNSPeL8SES+PiBeTJKbTI+JhYIukw9N13gFcl/Zb9WXgjRFxJHBtGmPBlIg4PiL+vYf36QI+R9IifbcS9qs3RwJnRsRbSJNtRBwGnA1cL2lMut7hwJuBw4A3S5qVzpsRES9Ot/lWCe9nZcCXoTac7IiIwkEWSecBz6lzkDSZ5MB6ZzrrOySDWgG8EvgSQETcL+n+dP4xJINe/T7pQopa4K7MyxY6HbwHeH0/4l0i6SPAOJLuJVYBvyAZX+Edkv6F5GB7FDAfeDFJD6GQdM/xVOa1fljkvb4PfEzSgZl5xfarN8sjYkf6/DiSJEZErJX0GHBIuuzWiNgCIGk1cEC6j3MlfRm4Efh1Ce9nZcDJwkYa0Xe30z0tE3BLRJzdyzaFvoM6KfF/Ij37/iqwOCKekHQ5UDgj/zFJn1y3AfdExDOS9gdW9XA1VLCtr/eLiA5J/w78n2wY9L5fHewpORjTbVn2vfrqwz3bp1InUBMRmyW9FHg1yVXJPwLv7Ct2Kw8uhrIRJe2Ge0vacSDAWzOLf1OYlvRioHAH0d3AsZIOTpeNk3QIL0zhALwxrSco9HRKRLQBN5MUaxWKadYBdZL+Lo1hlKRD+/me15H0KluXTve1X4+SFsMBb+jjNbOf2SEkndCt623ltOirKiJ+DHwceFk/98FGKCcLG4neAVyVVnDvyMy/GpiQFj99BPgTQEQ0A+cBP0iX3Q2UXJGdOlFSQ+FBMsDQ10l6uf0ZSTf4Wd8jHc0sjWEnSUL5rKT7gL8Af9+fANLX+BLpCGhF9uuTwH9K+i3JVUFvvgpUS3qApCjsvCK9tM4A7kjrlK4DLunPPtjI5V5nzXKQ3rk1OSI+PtSxmA0E11mYDTBJPwUOIhm9zaws+MrCzMyKcp2FmZkV5WRhZmZFOVmYmVlRThZmZlaUk4WZmRX1/wG4cYMSvc9hrwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1188607b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(H, acc_h)\n",
    "plt.xlabel('Hidden Layer Neurons')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Batch-Size=10, Learning Rate=0.0005')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how much adding information about time of day helps the network. Add a new set of inputs that represent the time of day. (Don't add information about day of week or absolute date.)  \n",
    "\n",
    "\n",
    "**PART 3.A**  \n",
    "\n",
    "Determine an appropriate representation for the time of day. Describe the representation you used. For example, you might add one unit with a value ranging from 0 to 1 for times ranging from 00:00 to 23:59. Report the representation you selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 3.A\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "for i in range(len(X.T[0])):\n",
    "    time = datetime.datetime.strptime(X.T[0][i].split()[1], '%H:%M:%S')\n",
    "    X.T[0][i] = abs(time-datetime.datetime(1970,1,1)).total_seconds()\n",
    "\n",
    "# Normalize the times using the minimum time\n",
    "minm = np.amin(X.T[0])\n",
    "X.T[0] = X.T[0] - minm\n",
    "maxm = np.amax(X.T[0])\n",
    "X.T[0] = X.T[0] / maxm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 3.B**  \n",
    "\n",
    "Train your net with $H=5$ hidden and compare training and test set performance to the net you built in (2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN ACCURACY\n",
      "\n",
      "H=5, Features=5\n",
      "0.9329967248908297 \n",
      "\n",
      "H=5, Features=6\n",
      "0.9312883435582822 \n",
      "\n",
      "\n",
      "TEST ACCURACY\n",
      "\n",
      "H=5, Features=5\n",
      "0.9288343558282208 \n",
      "\n",
      "H=5, Features=6\n",
      "0.9312883435582822 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Part 3.B\n",
    "\n",
    "mlp_time = MLP(n_vis=6, batch_size=10, lr=0.00025)\n",
    "mlp_time.train(X_train, y_train, 100)\n",
    "acc_time_test = mlp_time.test(X_test, y_test)\n",
    "acc_time_train = mlp_time.test(X_test, y_test)\n",
    "acc_train = mlp.test(X_train.T[1:].T, y_train)\n",
    "acc_test = mlp.test(X_test.T[1:].T, y_test)\n",
    "\n",
    "X = data[['date', 'Humidity', 'Light', 'CO2', 'HumidityRatio', 'Temperature']].values\n",
    "y = data['Occupancy'].values\n",
    "\n",
    "\n",
    "print (\"\\nTRAIN ACCURACY\\n\")\n",
    "print (\"H=5, Features=5\")\n",
    "print (acc_train,\"\\n\")\n",
    "print (\"H=5, Features=6\")\n",
    "print (acc_time_train,\"\\n\")\n",
    "print (\"\\nTEST ACCURACY\\n\")\n",
    "print (\"H=5, Features=5\")\n",
    "print (acc_test,\"\\n\")\n",
    "print (\"H=5, Features=6\")\n",
    "print (acc_time_test,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndg = np.random.RandomState(seed=0)\n",
    "epoch_list = []\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"\n",
    "        -- Skeleton Code for a Multi-Layer Perceptron Neural Net.\n",
    "           Network has 1 input layer followed by a hidden layer and\n",
    "           an output layer. \n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, n_vis=5, n_hid=10, n_out=1, batch_size=7, lr=0.1):\n",
    "        \"\"\"\n",
    "            Initialize an MLP object.\n",
    "            \n",
    "            params\n",
    "            \n",
    "            n_vis (int): # of neurons in input layer.\n",
    "            n_hid (int): # of neurons in hidden layer.\n",
    "            n_out (int): # number of neurons in final output layer.\n",
    "            batch_size (int): # number of X,y instances in a mini-batch.\n",
    "            lr (double): learning rate/step size.\n",
    "\n",
    "            class variables:\n",
    "            \n",
    "            _W1, _b1 (np.ndarray): \n",
    "                Weights and biases of the input layer respectively.\n",
    "            \n",
    "            _W2, _b2 (np.ndarray):\n",
    "                Weights and biases of the hidden layer respectively.\n",
    "            \n",
    "            _a01, _a12, _a23 (np.ndarray or None):\n",
    "                Activations from input, hidden and output layer respectively.  \n",
    "        \"\"\"\n",
    "        self._n_vis = n_vis\n",
    "        self._n_hid = n_hid\n",
    "        self._n_out = n_out\n",
    "        \n",
    "        self._W1 = rndg.normal(size=(self._n_hid, self._n_vis))\n",
    "        self._b1 = np.ones((self._n_hid, 1))\n",
    "\n",
    "        self._W2 = rndg.normal(size=(self._n_out, self._n_hid))\n",
    "        self._b2 = np.ones((self._n_out, 1))\n",
    "\n",
    "        self._a01 = None\n",
    "        self._a12 = None\n",
    "        self._a23 = None\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "            Sigmoid Logistic Function.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X (np.ndarray)\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            result (np.ndarray): result = f(X) \n",
    "                where f is the sigmoid function.\n",
    "            \n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-X.astype(float)))        \n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Feed forward an input through each \n",
    "            of the layers of network.\n",
    "            \n",
    "            Store each layer activation in the class \n",
    "            variables defined in the constructor. You'll\n",
    "            need them later during backpropogation.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X (np.ndarray): batch_size x 5 dimensional array\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            y_hat (np.ndarray): 1 x batch_size dimensional array\n",
    "                representing final layer outputs.\n",
    "            \n",
    "        \"\"\"\n",
    "        self._a01 = X.T\n",
    "        _z12 = np.dot(self._W1, self._a01) + self._b1\n",
    "        \n",
    "        self._a12 = self.sigmoid(_z12)\n",
    "        _z23 = np.dot(self._W2, self._a12) + self._b2\n",
    "        \n",
    "        self._a23 = self.sigmoid(_z23)\n",
    "        y_hat = self._a23\n",
    "        \n",
    "        return self._a23\n",
    "\n",
    "    def backward(self, y, y_hat):\n",
    "        \n",
    "        \"\"\"\n",
    "            Implement the backpropogation algorithm.\n",
    "            Assume mean squared loss at the output.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            y (np.ndarray): batch_size x 1 dimensional vector\n",
    "                of ground truth labels.\n",
    "            y_hat (np.nd_array): 1 x 7 dimensional vector\n",
    "                of predicted lables from forward().\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            l1_grad (np.ndarray): 1 x n_hid dimensional array\n",
    "                representing gradients for input layer.\n",
    "                \n",
    "            l2_grad (np.ndarray): n_hid x n_vis dimensional array\n",
    "                representing gradients for hidden layer.\n",
    "            \n",
    "        \"\"\"\n",
    "                \n",
    "        delta = (y_hat - y) * (y_hat*(1.0 - y_hat))\n",
    "\n",
    "        _a = self._a12\n",
    "        l2_grad = delta.dot(_a.T)\n",
    "        \n",
    "        delta_a = self._W2.T.dot(delta)\n",
    "        delta_sig = _a*(1.0 - _a)\n",
    "        delta = np.multiply(delta_a, delta_sig)\n",
    "        \n",
    "        _a = self._a01\n",
    "        l1_grad = delta.dot(_a.T)\n",
    "        \n",
    "        return l1_grad, l2_grad\n",
    "                \n",
    "    def update(self, l1_grad, l2_grad):\n",
    "        \"\"\"\n",
    "            Implement the update rule for network weights and biases.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            l1_grad (np.ndarray): gradients for input layer.\n",
    "            l2_grad (np.ndarray): gradients for hidden layer.\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            none.\n",
    "        \"\"\"\n",
    "        # TO-DO\n",
    "        self._W1 = self._W1 - self.lr * l1_grad\n",
    "        self._W2 = self._W2 - self.lr * l2_grad\n",
    "        \n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        \"\"\"\n",
    "            Returns one hot encoding of vector X using 0.5\n",
    "            as the threshold.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X (np.ndarray): predictions/activations of the output layer.\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            y (np.ndarray): one hot encoding of output layer.\n",
    "        \"\"\"\n",
    "        y_hat = self.forward(X)\n",
    "        for i in range(len(X)):\n",
    "            if y_hat[0][i] >= 0.5:\n",
    "                y_hat[0][i] = 1\n",
    "            else:\n",
    "                y_hat[0][i] = 0\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def train(self, X, y, epochs=1):\n",
    "        \"\"\"\n",
    "            Implement the train loop for n epochs.\n",
    "            In each epoch do the following:\n",
    "            1. Shuffle the dataset.\n",
    "            2. create self.batch_size sized-mini-batches of the dataset.\n",
    "            3. get network predictions using forward().\n",
    "            4. calculate the gradients using backward().\n",
    "            5. update the network weights using update().\n",
    "            6. Repeat 1-5 until convergence.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X (np.ndarray): N x 5 dimensional ndarray of inputs.\n",
    "            y (np.ndarray): N x 1 dimensional array of true labels.\n",
    "            epochs (int): # of epochs to train the network for. \n",
    "            \n",
    "            return:\n",
    "            \n",
    "            none.\n",
    "        \"\"\"\n",
    "        # TO-DO\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            X, y = shuffle(X, y)\n",
    "            \n",
    "            for j in range(0, X.shape[0], batch_size):\n",
    "                y_hat = self.forward(X[j:j+batch_size])\n",
    "                l1_grad, l2_grad = self.backward(y[j:j+batch_size], y_hat)\n",
    "                self.update(l1_grad, l2_grad)\n",
    "            \n",
    "            epoch_list.append(self.test(X_test.T[1:].T, y_test))\n",
    "            \n",
    "    def test(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "            Implement the test function which reports accuracy of \n",
    "            the model on the test set X_test against the true labels\n",
    "            y_test.\n",
    "            \n",
    "            You may re-use the predict and accuracy functions defined above.\n",
    "            \n",
    "            params:\n",
    "            \n",
    "            X_test (np.ndarray): N x M dimensional array where M is the \n",
    "                number of attributes considered for training the model.\n",
    "            y_test (np.ndarray): N x 1 dimensional array.\n",
    "            \n",
    "            return:\n",
    "            \n",
    "            accuracy (double): accuracy of the predicted labels against the \n",
    "                groundtruth labels.\n",
    "        \"\"\"\n",
    "        y = self.predict(X_test)\n",
    "        return self.accuracy(y_test, y)\n",
    "    \n",
    "    @classmethod\n",
    "    def accuracy(self, y, y_hat):\n",
    "        len_ = y.flatten().shape[0]\n",
    "        return np.sum(y.flatten() == y_hat.flatten())/len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:65: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.9116564417177914, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.7852760736196319, 0.9153374233128835, 0.9153374233128835, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9153374233128835, 0.9165644171779141, 0.9165644171779141, 0.9153374233128835, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9141104294478528, 0.9165644171779141, 0.9165644171779141, 0.912883435582822, 0.912883435582822, 0.9165644171779141, 0.9116564417177914, 0.9153374233128835, 0.9165644171779141, 0.9153374233128835, 0.9153374233128835, 0.9165644171779141, 0.9153374233128835, 0.9153374233128835, 0.9165644171779141, 0.9165644171779141, 0.9153374233128835, 0.9165644171779141, 0.9153374233128835, 0.9104294478527607, 0.9165644171779141, 0.9165644171779141, 0.9153374233128835, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9153374233128835, 0.9165644171779141, 0.9165644171779141, 0.9153374233128835, 0.9141104294478528, 0.9165644171779141, 0.9104294478527607, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9104294478527607, 0.9165644171779141, 0.9165644171779141, 0.9153374233128835, 0.9165644171779141, 0.9153374233128835, 0.9104294478527607, 0.9165644171779141, 0.9165644171779141, 0.9067484662576687, 0.9104294478527607, 0.9104294478527607, 0.9165644171779141, 0.9153374233128835, 0.9165644171779141, 0.9153374233128835, 0.9165644171779141, 0.9104294478527607, 0.9079754601226994, 0.9153374233128835, 0.9165644171779141, 0.9116564417177914, 0.9165644171779141, 0.9153374233128835, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9165644171779141, 0.9104294478527607, 0.9165644171779141, 0.9104294478527607, 0.9104294478527607, 0.912883435582822]\n"
     ]
    }
   ],
   "source": [
    "nn = MLP(batch_size=100, lr=0.0005)\n",
    "nn.train(X_train.T[1:].T, y_train, 100)\n",
    "print (epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
