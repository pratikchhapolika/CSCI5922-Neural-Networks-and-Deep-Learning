{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation (AD)\n",
    "***\n",
    "**Name**: __Poorwa Hirve__\n",
    "***\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this assignment is to build your own AD package from scratch in Python using NumPy for vector operations. At the core of this framework that you will be building, lie's a technique called _Operator Overloading_ . AD using the above techniques proceeds in 2 stages:\n",
    "\n",
    "1. Building a **Computation Graph** of operators and variables using **Tensor** and **Functions**. \n",
    "2. Evaluating the feed-forward and back-prop routine using the above computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Computational Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computational graph is a directed acyclic graph (DAG) where nodes correspond to operators or variables and edges represent the flow of data in between these nodes. Implementing a neural network as a computational graph (Torch, Chainer, Theano etc & c.) allows us to track operations executed in mapping a data set from $R^n \\to R^m$ with significantly less overhead. \n",
    "\n",
    "Moreover, once a computational graph has been built, back-propogation of gradients back to the inputs using the chain rule is quite efficient. \n",
    "\n",
    "<img src=\"./res/mlp_ann.png\" alt=\"mlp_ann\" style=\"width:500px;\"/>\n",
    "\n",
    "Through the course of this assignment, we realize a node in the computational graph using an object - `Function` [1] and implement a container for data called - `Tensor` [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Functions\n",
    "\n",
    "Every function/node is composed of the following 3 things:\n",
    "\n",
    "1. The `parents[]` which are inputs to that node. Each input is a Tensor object wrapping NumPy's ndarrays. \n",
    "2. The `forward()` which returns the result of an operator executed with its' `parents[]` as the input.\n",
    "3. The `backward()` which **accumulates** the gradient from it's children (output of forward()) and back-propogates the accumulated gradients to it's parents. The math for reverse mode accumulation will be implemented here.\n",
    "\n",
    "Below is an implementation of the `Function` class which we will be inheriting to implement following operations in this assignment.\n",
    "\n",
    "1. Add (Implementation and Usage provided)\n",
    "2. Subtract\n",
    "3. Multiply\n",
    "4. Divide\n",
    "5. Sum\n",
    "6. ReLU Activation\n",
    "7. Mean\n",
    "8. Dot Product\n",
    "9. Pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function(object):\n",
    "    \"\"\"\n",
    "    Represents a node in computational graph that perfoms \n",
    "    a computation.\n",
    "\n",
    "    During forward mode computation, it takes in \n",
    "    1 or more inputs/parents, and returns a result of the\n",
    "    computation as the output.\n",
    "\n",
    "    In reverse mode accumulation, it takes in the \n",
    "    gradients w.r.t. the output of the node, accumulates them\n",
    "    by calculating the gradients w.r.t it's inputs/parents\n",
    "    and back-propogates the gradients to the parents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of inputs to the node. \n",
    "    parents = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args):\n",
    "        \"\"\"\n",
    "        Forward mode computation of the 'operation'\n",
    "        on the inputs/parents to be implemented here.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Reverse mode computation of the node implemented here.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Function):\n",
    "    \"\"\"\n",
    "    Add parent inputs and return the result.\n",
    "    \"\"\"\n",
    "    def forward(self, *args):\n",
    "        \"\"\"\n",
    "        Forward computation of the binary Add operation.\n",
    "\n",
    "        param:\n",
    "        \n",
    "        args (n=2 Tensors): 2 Tensors to be added.\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        value (ndarray): Result of \"+\" operation on\n",
    "            input args.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Extend this nodes' parents to include new inputs. \n",
    "        self.parents = list(args)\n",
    "        # Add the 2 input Tensor's values\n",
    "        value = self.parents[0].value + self.parents[1].value\n",
    "        \n",
    "        return value\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Accumulates the gradients for \"Add\" operation from its'\n",
    "        children/outputs and passes them on to self.parents.\n",
    "        \n",
    "        param:\n",
    "        \n",
    "        gradient (ndarray or scalar): gradient w.r.t output of\n",
    "        \"Add\"\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Accumulate gradient. Notice how the gradients are accumulated\n",
    "        # and not stored in parents[i].grad directly. This is a key operation\n",
    "        # in reverse mode gradient accumulation\n",
    "        self.parents[0].grad += gradient\n",
    "        self.parents[1].grad += gradient\n",
    "        \n",
    "        # Back propogate gradients to parent's of each inputs.\n",
    "        # Stop when a leaf node is reached.\n",
    "        if not self.parents[0].is_leaf:\n",
    "             self.parents[0].backward(self.parents[0].grad)\n",
    "\n",
    "        if not self.parents[1].is_leaf:\n",
    "            self.parents[1].backward(self.parents[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sub(Function):\n",
    "    \"\"\"\n",
    "    Subtract parent inputs and return the result.\n",
    "    \"\"\"\n",
    "    def forward(self, *args):\n",
    "        \"\"\"\n",
    "        Forward computation of the binary Subtract operation.\n",
    "\n",
    "        param:\n",
    "        \n",
    "        args (n=2 Tensors): 2 Tensors to be subtracted.\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        value (ndarray): Result of \"-\" operation on\n",
    "            input args.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Extend this nodes' parents to include new inputs. \n",
    "        self.parents = list(args)\n",
    "        # Add the 2 input Tensor's values\n",
    "        value = self.parents[0].value - self.parents[1].value\n",
    "        return value\n",
    "        \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Accumulates the gradients for \"Subtract\" operation from its'\n",
    "        children/outputs and passes them on to self.parents.\n",
    "        \n",
    "        param:\n",
    "        \n",
    "        gradient (ndarray or scalar): gradient w.r.t output of\n",
    "        \"Sub\"\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        # Accumulate gradient. Notice how the gradients are accumulated\n",
    "        # and not stored in parents[i].grad directly. This is a key operation\n",
    "        # in reverse mode gradient accumulation\n",
    "        self.parents[0].grad += gradient\n",
    "        self.parents[1].grad -= gradient\n",
    "        \n",
    "        # Back propogate gradients to parent's of each inputs.\n",
    "        # Stop when a leaf node is reached.\n",
    "        if not self.parents[0].is_leaf:\n",
    "             self.parents[0].backward(self.parents[0].grad)\n",
    "\n",
    "        if not self.parents[1].is_leaf:\n",
    "            self.parents[1].backward(self.parents[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(Function):\n",
    "    \"\"\"\n",
    "    Multiply parent inputs and return the result.\n",
    "    \"\"\"\n",
    "    def forward(self, *args):\n",
    "        \"\"\"\n",
    "        Forward computation of the binary Multiply operation.\n",
    "        Hint: You can use np.multiply here\n",
    "        param:\n",
    "        \n",
    "        args (n=2 Tensors): 2 Tensors to be multiplied.\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        value (ndarray): Result of \"*\" operation on\n",
    "            input args.\n",
    "        \"\"\"\n",
    "        # Extend this nodes' parents to include new inputs. \n",
    "        self.parents = list(args)\n",
    "        # Add the 2 input Tensor's values\n",
    "        value = self.parents[0].value * self.parents[1].value\n",
    "        return value\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Accumulates the gradients for \"Multiply\" operation from its'\n",
    "        children/outputs and passes them on to self.parents.\n",
    "        \n",
    "        param:\n",
    "        \n",
    "        gradient (ndarray or scalar): gradient w.r.t output of\n",
    "        \"Mul\"\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        # Accumulate gradient. Notice how the gradients are accumulated\n",
    "        # and not stored in parents[i].grad directly. This is a key operation\n",
    "        # in reverse mode gradient accumulation\n",
    "        self.parents[0].grad += gradient * self.parents[1].value\n",
    "        self.parents[1].grad += gradient * self.parents[0].value\n",
    "        \n",
    "        # Back propogate gradients to parent's of each inputs.\n",
    "        # Stop when a leaf node is reached.\n",
    "        if not self.parents[0].is_leaf:\n",
    "             self.parents[0].backward(self.parents[0].grad)\n",
    "\n",
    "        if not self.parents[1].is_leaf:\n",
    "            self.parents[1].backward(self.parents[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Div(Function):\n",
    "    \"\"\"\n",
    "    Divide parent inputs and return the result.\n",
    "    \"\"\"\n",
    "    def forward(self, *args):\n",
    "        \"\"\"\n",
    "        Forward computation of the binary Divide operation.\n",
    "        \n",
    "        param:\n",
    "        \n",
    "        args (n=2 Tensors): 2 Tensors to be multiplied.\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        value (ndarray): Result of \"/\" operation on\n",
    "            input args.\n",
    "        \"\"\"\n",
    "        # Extend this nodes' parents to include new inputs. \n",
    "        self.parents = list(args)\n",
    "        # Add the 2 input Tensor's values\n",
    "        value = self.parents[0].value / self.parents[1].value\n",
    "        return value\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Accumulates the gradients for \"Divide\" operation from its'\n",
    "        children/outputs and passes them on to self.parents.\n",
    "        \n",
    "        param:\n",
    "        \n",
    "        gradient (ndarray or scalar): gradient w.r.t output of\n",
    "        \"Div\"\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        # Accumulate gradient. Notice how the gradients are accumulated\n",
    "        # and not stored in parents[i].grad directly. This is a key operation\n",
    "        # in reverse mode gradient accumulation\n",
    "        self.parents[0].grad += gradient / self.parents[1].value\n",
    "        self.parents[1].grad += -gradient * self.parents[0].value / (self.parents[1].value ** 2)\n",
    "        \n",
    "        # Back propogate gradients to parent's of each inputs.\n",
    "        # Stop when a leaf node is reached.\n",
    "        if not self.parents[0].is_leaf:\n",
    "             self.parents[0].backward(self.parents[0].grad)\n",
    "\n",
    "        if not self.parents[1].is_leaf:\n",
    "            self.parents[1].backward(self.parents[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum(Function):\n",
    "    \"\"\"\n",
    "    Implements Sum of a 1xN input vector and return the result of size 1X1.\n",
    "    \"\"\"\n",
    "    def forward(self, *args):\n",
    "        \"\"\"\n",
    "        Forward computation of the unary Sum operation.\n",
    "        Hint: You can use np.sum here\n",
    "        param:\n",
    "        \n",
    "        args (n=1 Tensor): Tensor whose elements are to be added.\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        value (ndarray): Result of \"Sum\" operation on\n",
    "            input args.\n",
    "        \"\"\"\n",
    "        # Extend this nodes' parents to include new inputs. \n",
    "        self.parents = list(args)\n",
    "        # Add the 2 input Tensor's values\n",
    "        value = np.sum(self.parents[0].value)\n",
    "        return value\n",
    "        \n",
    "    def backward(self, gradient = 1):\n",
    "        \"\"\"\n",
    "        Accumulates the gradients for \"Sum\" operation from its'\n",
    "        child/output and passes them on to self.parent.\n",
    "        \n",
    "        param:\n",
    "        \n",
    "        gradient (ndarray or scalar): gradient w.r.t output of\n",
    "        \"Sum\"\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        # Accumulate gradient. Notice how the gradients are accumulated\n",
    "        # and not stored in parents[i].grad directly. This is a key operation\n",
    "        # in reverse mode gradient accumulation\n",
    "        self.parents[0].grad += gradient\n",
    "        \n",
    "        # Back propogate gradients to parent's of each inputs.\n",
    "        # Stop when a leaf node is reached.\n",
    "        if not self.parents[0].is_leaf:\n",
    "             self.parents[0].backward(self.parents[0].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Function):\n",
    "    \"\"\"\n",
    "    ReLU parent input and return the result.\n",
    "    \"\"\"\n",
    "    def forward(self, *args):\n",
    "        \"\"\"\n",
    "        Forward computation of the unary ReLU operation.\n",
    "        Hint: You can use np.maximum here\n",
    "        param:\n",
    "        \n",
    "        args (n=1 Tensor): Tensor on which ReLU operation\n",
    "        is to be applied.\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        value (ndarray): Result of \"ReLU\" operation on\n",
    "            input args.\n",
    "        \"\"\"\n",
    "        # Extend this nodes' parents to include new inputs. \n",
    "        self.parents = list(args)\n",
    "        # Add the 2 input Tensor's values\n",
    "        value = np.maximum(0, self.parents[0].value)\n",
    "        return value\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Accumulates the gradients for \"ReLU\" operation from its'\n",
    "        child/output and passes them on to self.parent.\n",
    "        \n",
    "        param:\n",
    "        \n",
    "        gradient (ndarray or scalar): gradient w.r.t output of\n",
    "        \"ReLU\"\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        # Accumulate gradient. Notice how the gradients are accumulated\n",
    "        # and not stored in parents[i].grad directly. This is a key operation\n",
    "        # in reverse mode gradient accumulation\n",
    "        \n",
    "        for i in range(len(self.parents[0].value)):\n",
    "            self.parents[0].grad[i] += gradient[i] if self.parents[0].value[i] > 0 else 0\n",
    "        \n",
    "        # Back propogate gradients to parent's of each inputs.\n",
    "        # Stop when a leaf node is reached.\n",
    "        if not self.parents[0].is_leaf:\n",
    "             self.parents[0].backward(self.parents[0].grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.7 Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mean(Function):\n",
    "    \"\"\"\n",
    "    Implements Mean of a 1xN input vector and return the result of size 1X1.\n",
    "    \"\"\"\n",
    "    def forward(self, *args):\n",
    "        \"\"\"\n",
    "        Forward computation of the unary Mean operation.\n",
    "        Hint: You can use np.mean here\n",
    "        param:\n",
    "        \n",
    "        args (n=1 Tensor): Tensor on which Mean operation\n",
    "        is to be applied.\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        value (ndarray): Result of \"Mean\" operation on\n",
    "            input args.\n",
    "        \"\"\"\n",
    "        # Extend this nodes' parents to include new inputs. \n",
    "        self.parents = list(args)\n",
    "        # Add the 2 input Tensor's values\n",
    "        value = np.mean(self.parents[0].value)\n",
    "        return value\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Accumulates the gradients for \"Mean\" operation from its'\n",
    "        child/output and passes them on to self.parent.\n",
    "        \n",
    "        param:\n",
    "        \n",
    "        gradient (ndarray or scalar): gradient w.r.t output of\n",
    "        \"Mean\"\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        # Accumulate gradient. Notice how the gradients are accumulated\n",
    "        # and not stored in parents[i].grad directly. This is a key operation\n",
    "        # in reverse mode gradient accumulation\n",
    "        self.parents[0].grad += gradient / len(self.parents[0].value)\n",
    "        \n",
    "        # Back propogate gradients to parent's of each inputs.\n",
    "        # Stop when a leaf node is reached.\n",
    "        if not self.parents[0].is_leaf:\n",
    "             self.parents[0].backward(self.parents[0].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8 Pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pow(Function):\n",
    "    \"\"\"\n",
    "    Implements Power operation on a 1xN input vector.\n",
    "    Raises the input to an exponent \n",
    "    \"\"\"\n",
    "    exp = 0\n",
    "    def forward(self, *args, exp):\n",
    "        \"\"\"\n",
    "        Forward computation of the bianry Power operation.\n",
    "        Hint: You can use np.power here\n",
    "        param:\n",
    "        \n",
    "        args (n=1 Tensor): Tensor on which Power operation\n",
    "        is to be applied.\n",
    "        \n",
    "        exp (int): Exponent of the power function\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        value (ndarray): Result of \"Power\" operation on\n",
    "            input args.\n",
    "        \"\"\"\n",
    "        # Extend this nodes' parents to include new inputs. \n",
    "        self.parents = list(args)\n",
    "        self.exp = exp\n",
    "        # Add the 2 input Tensor's values\n",
    "        value = np.power(self.parents[0].value, exp)\n",
    "        return value\n",
    "        \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Accumulates the gradients for \"Power\" operation from its'\n",
    "        child/output and passes them on to self.parent.\n",
    "        \n",
    "        param:\n",
    "        \n",
    "        gradient (ndarray or scalar): gradient w.r.t output of\n",
    "        \"Pow\"\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        # Accumulate gradient. Notice how the gradients are accumulated\n",
    "        # and not stored in parents[i].grad directly. This is a key operation\n",
    "        # in reverse mode gradient accumulation\n",
    "        \n",
    "        power = self.exp\n",
    "        self.parents[0].grad += gradient * power * (np.power(self.parents[0].value,power-1))\n",
    "        \n",
    "        # Back propogate gradients to parent's of each inputs.\n",
    "        # Stop when a leaf node is reached.\n",
    "        if not self.parents[0].is_leaf:\n",
    "             self.parents[0].backward(self.parents[0].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8 Dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dot(Function):\n",
    "    \"\"\"\n",
    "    Computes the dot product of the 2 parent inputs.\n",
    "    \"\"\"\n",
    "    def forward(self, *args):\n",
    "        \"\"\"\n",
    "        Forward computation of the bianry Dot operation.\n",
    "        Hint: You can use np.dot here\n",
    "        param:\n",
    "        \n",
    "        args (n=2 Tensors): Tensors on which Dot operation\n",
    "        is to be performed.\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        value (ndarray): Result of \"Dot\" operation on\n",
    "            input args.\n",
    "        \"\"\"\n",
    "        # Extend this nodes' parents to include new inputs. \n",
    "        self.parents = list(args)\n",
    "        # Add the 2 input Tensor's values\n",
    "        value = np.dot(self.parents[0].value, self.parents[1].value)\n",
    "        return value\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Accumulates the gradients for \"Dot\" operation from its'\n",
    "        children/outputs and passes them on to self.parent.\n",
    "        \n",
    "        param:\n",
    "        \n",
    "        gradient (ndarray or scalar): gradient w.r.t output of\n",
    "        \"Dot\"\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        # Accumulate gradient. Notice how the gradients are accumulated\n",
    "        # and not stored in parents[i].grad directly. This is a key operation\n",
    "        # in reverse mode gradient accumulation\n",
    "        self.parents[0].grad += gradient * self.parents[1].value\n",
    "        self.parents[1].grad += gradient * self.parents[0].value\n",
    "        \n",
    "        # Back propogate gradients to parent's of each inputs.\n",
    "        # Stop when a leaf node is reached.\n",
    "        if not self.parents[0].is_leaf:\n",
    "             self.parents[0].backward(self.parents[0].grad)\n",
    "\n",
    "        if not self.parents[1].is_leaf:\n",
    "            self.parents[1].backward(self.parents[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tensor\n",
    "\n",
    "Here, we create a new variable type - **Tensor** that implements all the common primitive operations of numeric types such as `+, -, *, /` as well as some composite functions such as `pow`, `sum`, `mean`, `dot`. To facilitate matrix operations, a **Tensor** wraps around NumPy's ndarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "    \"\"\"\n",
    "    Tensor. A wrapper around NumPy's vectors \n",
    "    which supports a backward call.\n",
    "    \n",
    "    Uses operations defined in section 1.1 which takes Tensors\n",
    "    as arguments and returns a new Tensor with reference to the\n",
    "    operations's gradient function.\n",
    "    \"\"\"\n",
    "    def __init__(self, value, is_leaf = True, grad_fn = None):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \n",
    "        Every Tensor which is a result of the operations in section\n",
    "        1.1 is a non leaf node.\n",
    "        A non leaf node holds reference to the backward() of a \n",
    "        Function class by which the Tensor has been created. This is\n",
    "        how the computational graph is built.\n",
    "        \n",
    "        params:\n",
    "        \n",
    "        value (ndarray or scalar): Holds actual data.\n",
    "        \n",
    "        is_leaf (bool): Specifies if the Tensor is a leaf node.\n",
    "        \n",
    "        grad_fn (Function.backward method): The gradient object which holds forward and \n",
    "            backward calls specific to the operation that results in the creation\n",
    "            of this Tensor. \n",
    "        \n",
    "        member variables:\n",
    "        \n",
    "        value (ndarray): value in the arguments to the is converted to\n",
    "            an ndarray to support numpy vectorization.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if grad_fn is None and not is_leaf:\n",
    "            raise ValueError(\n",
    "                'Non leaf nodes require a grad_fn.'\n",
    "            )\n",
    "        \n",
    "        if np.isscalar(value):\n",
    "            value = np.ones(1)*value\n",
    "        \n",
    "        if not isinstance(value, np.ndarray):\n",
    "            raise ValueError(\n",
    "                'Value should be of type \"np.ndarray\" or a scalar, but received {type(value)}'\n",
    "            )\n",
    "        \n",
    "        self.value = value\n",
    "        self.is_leaf = is_leaf\n",
    "        self.grad_fn = grad_fn\n",
    "        self.zero_grad()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Reset the gradients of this Tensor to 0 taking in consideration\n",
    "        the dimensions of the data stored by it.\n",
    "        \"\"\"\n",
    "        self.grad = np.zeros(self.value.shape)\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Initiates the chain rule on the computational graph.\n",
    "        \"\"\"\n",
    "        self.grad = gradient\n",
    "        self.grad_fn(self.grad)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Overloaded \"+\" primitive.\n",
    "        Example:\n",
    "        c = a+b\n",
    "        params:\n",
    "\n",
    "        self (Tensor): denoted by 'a' in the example expression\n",
    "        other (Tensor): denoted by 'b' in the example expression\n",
    "\n",
    "        returns:\n",
    "\n",
    "        Tensor: denoted by 'c' in the above expression.\n",
    "        \"\"\"    \n",
    "        function = Add()\n",
    "        forward_value = function.forward(self, other)\n",
    "        \n",
    "        return Tensor(\n",
    "            value = forward_value,\n",
    "            is_leaf = False,\n",
    "            grad_fn = function.backward\n",
    "        )\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"\"\"\n",
    "        Overloaded \"-\" primitive.\n",
    "        Example:\n",
    "        c = a-b\n",
    "        params:\n",
    "\n",
    "        self (Tensor): denoted by 'a' in the example expression\n",
    "        other (Tensor): denoted by 'b' in the example expression\n",
    "\n",
    "        returns:\n",
    "\n",
    "        Tensor: denoted by 'c' in the above expression.\n",
    "        \"\"\"\n",
    "        function = Sub()\n",
    "        forward_value = function.forward(self, other)\n",
    "        \n",
    "        return Tensor(\n",
    "            value = forward_value,\n",
    "            is_leaf = False,\n",
    "            grad_fn = function.backward\n",
    "        )\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        Overloaded \"*\" primitive.\n",
    "        Example:\n",
    "        c = a*b\n",
    "        params:\n",
    "\n",
    "        self (Tensor): denoted by 'a' in the example expression\n",
    "        other (Tensor): denoted by 'b' in the example expression\n",
    "\n",
    "        returns:\n",
    "\n",
    "        Tensor: denoted by 'c' in the above expression.\n",
    "        \"\"\"\n",
    "        function = Mul()\n",
    "        forward_value = function.forward(self, other)\n",
    "        \n",
    "        return Tensor(\n",
    "            value = forward_value,\n",
    "            is_leaf = False,\n",
    "            grad_fn = function.backward\n",
    "        )\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "\n",
    "        \"\"\"\n",
    "        Overloaded \"/\" primitive.\n",
    "        Example:\n",
    "        c = a/b\n",
    "        params:\n",
    "\n",
    "        self (Tensor): denoted by 'a' in the example expression\n",
    "        other (Tensor): denoted by 'b' in the example expression\n",
    "\n",
    "        returns:\n",
    "\n",
    "        Tensor: denoted by 'c' in the above expression.\n",
    "        \"\"\"\n",
    "        function = Div()\n",
    "        forward_value = function.forward(self, other)\n",
    "        \n",
    "        return Tensor(\n",
    "            value = forward_value,\n",
    "            is_leaf = False,\n",
    "            grad_fn = function.backward\n",
    "        )\n",
    "    \n",
    "    def sum(self):\n",
    "        \"\"\"\n",
    "        \"Sum elements of this Tensor\"\n",
    "        Example:\n",
    "        b = a.sum()\n",
    "        params:\n",
    "\n",
    "        self (Tensor): denoted by 'a' in the example expression\n",
    "        \n",
    "        returns:\n",
    "\n",
    "        Tensor: denoted by 'b' in the above expression.\n",
    "        \"\"\"\n",
    "        function = Sum()\n",
    "        forward_value = function.forward(self)\n",
    "        \n",
    "        return Tensor(\n",
    "            value = forward_value,\n",
    "            is_leaf = False,\n",
    "            grad_fn = function.backward\n",
    "        )\n",
    "    \n",
    "    def relu(self):\n",
    "        \"\"\"\n",
    "        \"ReLU activation applied on this Tensor\"\n",
    "        Example:\n",
    "        b = a.relu()\n",
    "        params:\n",
    "\n",
    "        self (Tensor): denoted by 'a' in the example expression\n",
    "        \n",
    "        returns:\n",
    "\n",
    "        Tensor: denoted by 'b' in the above expression.\n",
    "        \"\"\"\n",
    "        function = ReLU()\n",
    "        forward_value = function.forward(self)\n",
    "        \n",
    "        return Tensor(\n",
    "            value = forward_value,\n",
    "            is_leaf = False,\n",
    "            grad_fn = function.backward\n",
    "        )\n",
    "    \n",
    "    def mean(self):\n",
    "        \"\"\"\n",
    "        \"Mean of element of this Tensor\"\n",
    "        Example:\n",
    "        b = a.mean()\n",
    "        params:\n",
    "\n",
    "        self (Tensor): denoted by 'a' in the example expression\n",
    "        \n",
    "        returns:\n",
    "\n",
    "        Tensor: denoted by 'b' in the above expression.\n",
    "        \"\"\"\n",
    "        function = Mean()\n",
    "        forward_value = function.forward(self)\n",
    "        \n",
    "        return Tensor(\n",
    "            value = forward_value,\n",
    "            is_leaf = False,\n",
    "            grad_fn = function.backward\n",
    "        )\n",
    "    \n",
    "    def pow(self, exp):\n",
    "        \"\"\"\n",
    "        \"Raise the value in this Tensor to an exponent\"\n",
    "        Example:\n",
    "        b = a.pow(exp)\n",
    "        params:\n",
    "\n",
    "        self (Tensor): denoted by 'a' in the example expression\n",
    "        exp (int): denoted by 'exp' in the example expression\n",
    "        \n",
    "        returns:\n",
    "\n",
    "        Tensor: denoted by 'b' in the above expression.\n",
    "        \"\"\"\n",
    "        function = Pow()\n",
    "        forward_value = function.forward(self, exp=exp)\n",
    "        \n",
    "        return Tensor(\n",
    "            value = forward_value,\n",
    "            is_leaf = False,\n",
    "            grad_fn = function.backward\n",
    "        )\n",
    "    \n",
    "        \n",
    "    def dot(self, other):\n",
    "        \"\"\"\n",
    "        Dot product of 2 Tensors\n",
    "        Example:\n",
    "        c = a.dot(b)\n",
    "        params:\n",
    "\n",
    "        self (Tensor): denoted by 'a' in the example expression\n",
    "        other (Tensor): denoted by 'b' in the example expression\n",
    "\n",
    "        returns:\n",
    "\n",
    "        Tensor: denoted by 'c' in the above expression.\n",
    "        \"\"\"\n",
    "        function = Dot()\n",
    "        forward_value = function.forward(self, other)\n",
    "        \n",
    "        return Tensor(\n",
    "            value = forward_value,\n",
    "            is_leaf = False,\n",
    "            grad_fn = function.backward\n",
    "        )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'value: {}, grad: {}, grad_fn = {}'.format(\n",
    "            self.value, self.grad, self.grad_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "........\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.228s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=8 errors=0 failures=0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestAD(unittest.TestCase):\n",
    "    \n",
    "    a = np.array([-0.09944354, -1.00754719, -0.9861803 , -1.19309044,  0.52475517])\n",
    "    b = np.array([ 0.07470977, -0.40344353,  1.30297429,  1.2697801 ,  0.79662585])\n",
    "    \n",
    "    def test_mul(self):\n",
    "        x = Tensor(self.a)\n",
    "        y = Tensor(self.b)\n",
    "        \n",
    "        res = (x*y).sum()\n",
    "        res.backward(1.0)\n",
    "        \n",
    "        exp = np.array([-1.98283756])\n",
    "        exp_grad_x = np.array([ 0.07470977, -0.40344353,  1.30297429,  1.2697801, 0.79662585])\n",
    "        exp_grad_y = np.array([-0.09944354, -1.00754719, -0.9861803,  -1.19309044,  0.52475517])\n",
    "        \n",
    "        self.assertTrue(np.allclose(exp, res.value))\n",
    "        self.assertTrue(np.allclose(exp_grad_x, x.grad))\n",
    "        self.assertTrue(np.allclose(exp_grad_y, y.grad))\n",
    "    \n",
    "    def test_add_sum(self):\n",
    "        x = Tensor(self.a) \n",
    "        y = Tensor(self.b)\n",
    "        \n",
    "        res = (x+y).sum()\n",
    "        res.backward(1.0)\n",
    "        \n",
    "        exp = np.array([0.27914019])\n",
    "        exp_grad_x = np.ones_like(self.a)\n",
    "        exp_grad_y = np.ones_like(self.b)\n",
    "        \n",
    "        self.assertTrue(np.allclose(exp, res.value))\n",
    "        self.assertTrue(np.allclose(exp_grad_x, x.grad))\n",
    "        self.assertTrue(np.allclose(exp_grad_y, y.grad))\n",
    "    \n",
    "    def test_sub(self):\n",
    "        x = Tensor(self.a)\n",
    "        y = Tensor(self.b)\n",
    "        \n",
    "        res = (x-y).sum()\n",
    "        res.backward(1.0)\n",
    "        \n",
    "        exp = np.array([-5.80215279])\n",
    "        exp_grad_x = np.ones_like(self.a)\n",
    "        exp_grad_y = -np.ones_like(self.b)\n",
    "        \n",
    "        self.assertTrue(np.allclose(exp, res.value))\n",
    "        self.assertTrue(np.allclose(exp_grad_x, x.grad))\n",
    "        self.assertTrue(np.allclose(exp_grad_y, y.grad))\n",
    "    \n",
    "    def test_truediv(self):\n",
    "        x = Tensor(self.a)\n",
    "        y = Tensor(self.b)\n",
    "        \n",
    "        res = (x/y).sum()\n",
    "        res.backward(1.0)\n",
    "        \n",
    "        exp = np.array([0.12855352])\n",
    "        \n",
    "        exp_grad_x = np.asarray([13.38513022, -2.47866164,  0.76747485,  0.78753794,  1.25529444])\n",
    "        exp_grad_y = np.asarray([17.81647477,  6.19013165,  0.5808776 ,  0.73997378, -0.82689037])\n",
    "        \n",
    "        self.assertTrue(np.allclose(exp, res.value))\n",
    "        self.assertTrue(np.allclose(exp_grad_x, x.grad))\n",
    "        self.assertTrue(np.allclose(exp_grad_y, y.grad))\n",
    "        \n",
    "    def test_pow(self):\n",
    "        x = Tensor(self.a)\n",
    "        \n",
    "        res = (x.pow(5)).sum()\n",
    "        res.backward(1.0)\n",
    "        \n",
    "        exp = np.array([-4.34881576])\n",
    "        exp_grad_x = np.array([0.00048896, 5.15266121, 4.72928291, 10.13126015, 0.37913764])\n",
    "        self.assertTrue(np.allclose(exp, res.value))\n",
    "        self.assertTrue(np.allclose(exp_grad_x, x.grad))\n",
    "        \n",
    "    def test_dot(self):\n",
    "        x = Tensor(self.a)\n",
    "        y = Tensor(self.b)\n",
    "        \n",
    "        res = x.dot(y)\n",
    "        res.backward(1.0)\n",
    "        \n",
    "        exp = np.array([-1.98283756])\n",
    "        \n",
    "        exp_grad_x = np.array([ 0.07470977, -0.40344353, 1.30297429, 1.2697801, 0.79662585])\n",
    "        exp_grad_y = np.array([-0.09944354, -1.00754719, -0.9861803, -1.19309044, 0.52475517])\n",
    "        \n",
    "        self.assertTrue(np.allclose(exp, res.value))\n",
    "        self.assertTrue(np.allclose(exp_grad_x, x.grad))\n",
    "        self.assertTrue(np.allclose(exp_grad_y, y.grad))\n",
    "        \n",
    "    def test_relu(self):\n",
    "        x = Tensor(self.a)\n",
    "        y = Tensor(self.b)\n",
    "        \n",
    "        res = (x*y).relu().sum()\n",
    "        res.backward(1.0)\n",
    "        \n",
    "        exp = np.array([0.82452193])\n",
    "        exp_grad_x = np.array([0, -0.40344353, 0, 0, 0.79662585])\n",
    "        exp_grad_y = np.array([0, -1.00754719, 0, 0, 0.52475517])\n",
    "        \n",
    "        self.assertTrue(np.allclose(exp, res.value))\n",
    "        self.assertTrue(np.allclose(exp_grad_x, x.grad))\n",
    "        self.assertTrue(np.allclose(exp_grad_y, y.grad))\n",
    "        \n",
    "    def test_mean(self):\n",
    "        x = Tensor(self.a)\n",
    "        y = Tensor(self.b)\n",
    "        \n",
    "        res = (x+y).mean().sum()\n",
    "        res.backward(1.0)\n",
    "        \n",
    "        exp = np.array([0.05582804])\n",
    "        exp_grad_x = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "        exp_grad_y = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "        \n",
    "        self.assertTrue(np.allclose(exp, res.value))\n",
    "        self.assertTrue(np.allclose(exp_grad_x, x.grad))\n",
    "        self.assertTrue(np.allclose(exp_grad_y, y.grad))\n",
    "    \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestAD)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
